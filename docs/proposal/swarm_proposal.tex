\documentclass[]{article}

\usepackage{xargs} 
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}

%opening
\title{Command Language for Single-User, Multi-Robot Search}
\author{Abraham Shultz}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

Methods for command and control that are based on issuing individual orders to individual actors, whether they are robotic or human, do not scale to large numbers of actors. 
In human organizations, this problem is solved by creating hierarchies of humans to propagate commands, with the attendant increase in cost and potential to introduce errors. 
By defining a mapping from user interface gestures to individual programs loaded on local robots, we can disintermediate the command structure, and allow an individual to control arbitrarily large, heterogeneous swarms.
In order to remain robust in the face of failure, the overall action of the swarm should be decentralized emergent behavior, rather than a centralized orchestration. 
Each robot receives its own program, and the sum of the execution of the programs on each robot results in task completion as emergent property.
Because a robotic swarm's situation develops over time, there is no such thing as a homogeneous swarm.
Each robot is at a distinct location and has a distinct level of resources such as remaining battery charge. 
As a consequence, robot selection for tasks in a heterogeneous swarm is best deferred until compile-time.

\section{Research Questions}

In order to create a mapping from a command language for multi-robot command and control, the command language must first be specified. 
The command language in this work is a system of common gestures as determined empirically in \cite{Micire:2009:ANG:1731903.1731912}. 
%Is there a coherent command language for controlling large swarms?  
For the purposes of this research, a coherent command language is one that is found to be intuitive by the majority of users. 
In \cite{Micire:2009:ANG:1731903.1731912}, for each available command, one or two gestures were used by \%60 of the users. 
In contrast, an incoherent language will have very little commonality of gestures used between users. 
Defining the scale required for a swarm to be considered ``large'' will be done empirically.
It is expected that there exists a transition point for the number of members in a swarm where users will stop interacting with the UI representation of the members of the swarm as individuals, and attempt to interact with the representations as groups or collections. 
For example, rather than selecting each robot by clicking on it, the may ``paint'' over the area containing the robots they want to use. 
A large swarm is, then, a swarm with a number of members above the point at which such a transition occurs. 

Once the language is defined, sequences of commands in that language must be translated into programs to execute on each member of the swarm. 
The program for each swarm member must be generated automatically, either by creation of a new program or by a synthetic approach from per-determined behavior primitives.
Because the motivating examples used in swarm robotic research are almost never related to swarm software development, the end user cannot be expected to do the programming themselves. 
All of the valid expressions possible in the command language should be converted into programs for the robots, or the user must be usefully informed as to why it was not possible. 
The synthesized program should result in convergence of the swarm's overall behavior to the desired result. 
Clearly, in a developing situation in the real world, success may be impossible, and so there is not a practical way to guarantee that a particular valid command sequence will result in a particular desired state of the world. 
However, certain minimum bounds on the problem may be able to be used to determine if a desired task is certain to fail.

Emergent behaviors arise from the interactions of actors with each other and the world around them. 
In the face of uncertainty in the world, the behaviors will also become uncertain. 
Programs synthesized to guide the swarm should be designed to be robust against failure or degradation of swarm members. 
The heterogeneity of the swarm may also be leveraged to increase its robustness against failures of individual nodes or alterations of the environment. 
Because heterogeneity increases the dimensionality of the solution space for program synthesis, it may adversely affect the performance of the program synthesis and the swarm's runtime convergence to the desired state.


\section{Work Plan}

\subsection{Build a swarm}

In order to experiment with swarm robotics, a swarm must be made.
Much can be done in simulation, but it is a daunting prospect to simulate the dynamics of real robot motion and the changes in the performance of each swarm member as mechanical wear and other forces influence them. 
Further, swarm simulations must not have bugs in the software which results in incorrect behavior. 
Genetic algorithms are infamous for exploiting quirks of their simulation environment which are not present in the real world, but still allow members of the population to inflate their fitness. 
Any behavior of a physical, unsimulated swarm is the real behavior of the swarm, and cannot be blamed on a defect in the simulation of the swarm or its environment. 

Because the focus of this work is to determine how gesture control scales or fails to scale, and humans primary mode of gesturing is with their hands, the size of the swarm should be significantly larger than the number of fingers a user could gesture with. 

\subsection{Swarm Robot Hardware}

Swarm robots are generally small. 
The reason to keep swarm robots small is two-fold. 
First, larger robots consume more materials per unit, and so costs more money.
As a result, for a given number of swarm units, larger robots will result in a higher cost swarm. 
Second, each robot requires some amount of space to move around in. 
To keep the ratio of free space to robots constant, the area of space used by the robots grows as the robots do. 
If the ratio isn't kept constant, the robots will crowd each other, and so large robots will require either a very large space, or become overly crowded.

The challenge of construction of swarm robots hardware, then, is to put all of the same parts as non-swarm mobile robots: a mobility platform, a processor, some sensors, and a communication system, into a small package.  
The robots described in this work accomplish this task by using a Commercial Off-The-Shelf (COTS) module to provide Wi-Fi connectivity and a microcontroller for processing. 
The module used in this swarm hardware is a ESP8266-03  \unsure{Get a picture of the ESP8266}, which provides a Wi-Fi interface and approximately 500kB of flash memory for programs. 
The ESP8266 is avialable in several form factors, each designated by a different suffix. 
The 03 version was chosen because it offers more GPIO pins than most other versions, and includes an internal antenna, which some versions lack. 

This swarm controller module was designed to be used as a replacement for the control electronics of children's toys, similar to the \unsure{Cite Ionas} swarm robots developed in PAPER. 
Most children's toys use either one motor with a mechanical linkage to cause the toy to turn when the motor is reversed, or two motors.
Two-motor toys frequently use either differential steering or have one motor provide drive power and the other provide steering. 

Unfortunately, such small processors do not have significant computational power. 
The majority of the processing will be performed on a host computer running the ROS software framework. 
However, the host computer will not be a central control node for the swarm. 
Instead, the host computer will include a separate process for each robot in the swarm, which will be allowed to drive only the robot that it is linked to. 
Each of these robot processes will effectively act as a local control program for the robot, but will have the full processing resources of the host computer. 
As a result, the individual robots can be small, lightweight, and relatively low power, but the system as a whole will endow them with significant computing power. 

Software for the swarm, then, will be of two classes. 
The first class is infrastructure software that provides things like a medium for communication between the robots, or emulating proximity sensing for each robot. 
Having communication mediated by software on the host will allow for simple experimentation with variable network bandwidth or reliability. 
The second class of software is the program that controls each robot. 
The system as a whole will not constrain the robot control programs to be identical across all robots, but it may be useful to constrain the robot control programs to be the same for certain experiments. 

Currently existing swarm robots are too expensive to build a large swarm, with the exception of the Harvard Kilobots. 
The Harvard Kilobots do not support hardware heterogeneity, but in real world applications, robots will become heterogeneous by interaction with the world. 
Wear and tear on the robots will result in differences between the robots, even if they had started from identical conditions. 
Even sending part of a swarm up a hill while the rest remain at the bottom will result in the higher members of the swarm having less battery life than those that didn't move. 
Battery life may be relevant to allocation of tasks, meaning that the physical elevation gradient of the robots becomes a gradient of fitness for a task as well. 

However, heterogeneity can also be leveraged to increase the robustness of a swarm and reduce its cost. 
A hetergeneous swarm containing both flying and ground robots can allow specialization of the individual components. 
By way of historical analogy, airplanes and cars have been separate types of vehicles, each specialized for a particular task. 
Flying cars, however, are normally either poor cars, poor planes, or both, because the demands of each task are different. 
Cars require a certain amount of weight and downward force for traction, but weight and downward air forces are the enemy of developing a good airplane. 
Rather than requiring that all robots be able to traverse all terrains, the ground robots can be developed to cover ground well, while the aerial robots are optimized for longer ranges or dwell times. 
Heterogeneity also allows the robots to be cheaper, because not every robot has to do everything.
Combining robots that can fly and robots that can roll allows a mixture of overview sensing and mission endurance that would be difficult to produce robustly and inexpensively with robots that can do both. 

In order to be both heterogeneous and inexpensive, the robots used for this work will be constructed by developing a consistent control hardware platform that can be attached to children's toys. 
Each controller will use a ESP-8266 wifi module for wireless communication and as a micro controller. 
The ESP-8266 costs approximately \$3-5, and contains both a wireless interface and a micro controller that can be programmed from a variety of programming environments and languages, including Lua and the Arduino variant of C/C++. 

In order to locate the robots within the experiment area, an overhead camera is used to detect QR codes on each robot. 
The QR codes provide location, heading, and a unique identifier for each robot. 
In this system, the central computer provides virtualization of the processing on each robot. 
Virtualizing the computational resources on each robot effectively allows us to borrow robots from the future. 
As technology develops, higher powered computers and and more capable sensors have become cheaper and smaller. 
Allowing each swarm node to have a virtual computer ``riding'' on it gives us as much computational power as we want at each point in the swarm, even if hardware of that power level is not currently available.
The robots themselves are effectively Internet of Things (IoT) nodes with the ability to move around. 
This is, then, an extension of the work done with mobile sensor motes in PAPER

The decision to use ESP-8266 wifi modules as the core of the swarm robots designed in this work is an extension of a long legacy of previous robotics projects. 

%detail the other projects here, chronologically
The use of COTS hardware in research robotics has lead to at least two platforms refered to as COTSBots.
Bergbreiter's COTSBots used mote hardware for the communications link and sensing, plus a motor control add-on board  \cite{bergbreiter2003cotsbots}. 
The mobility platform is a hacked toy, in particular, a specific brand of high-quality micro RC car.
At the time of this writing, the particular car used is moderately expensive for a toy car, although quite cheap for a research robot, costing a over \$100USD per unit. 
Bergbreiter's COTSBots use TinyOS, a modular and event-driven framework for developing node software. 
ROS also provides a modular, event-driven framework, so many of the design ideas, if not the code, are equally applicable to the system described in this paper. 
TinyOS is written in a dialect of C called nesC rather than ROS's polyglot approach. The motor and mote boards communicate using a messaging layer, again like ROS. 
The motor driver board is not commercially available, but can be custom-built by board fabrication companies, without the researcher having to assemble it by hand. 
Ohio State also developed a very small microwave RADAR that can go on the boards.

The second version of COTSBots arrived 8 years later, in ``COTSBots: Computationally Powerful, Low-Cost Robots for Computer Science Curriculums" \cite{soule2011cotsbots}. Soule and Heckendorne describe a platform composed of a laptop, which controls a modified RC car, tank, or similar toy through some combination of motor drivers. 
Due to the diversity of possible combinations of hardware that can be assembled into this configuration, it is still a very viable platform. 
However, the minimum size of this style of COTSBot is the size of a laptop, which is in turn dictated by the minimum size of a useful keyboard. 
The large size of these COTSBots demands a very large space if the density of robots in a large swarm is to be kept low. 
Additionally, each laptop has a screen, keyboard, and so forth that are not useful while the robot is operating. 
All of these parts add to the overall cost of the swarm. 

Many impressive designs for swarm robot platforms have been proposed, and constructed, but are no longer easily commercially available, or never were. 

At the low end, in terms of scale, the I-SWARM Project intended to create a 2x2x1mm robot that moved by stick-slip locomotion actuated by piezo levers\cite{seyfried2005swarm}. Over the course of the project from 2004-2008, the hardware was developed and used in research, but was not converted to a commercial product. 

Alice, by Caprari et al. packed a PIC16F84 processor, motors, RF and IR networking, and enough battery power for 10 hours of autonomy into a robot measuring under one cubic inch \cite{caprari1998autonomous}. Unfortunately, the processor is anemic by modern standards, and the platform as a whole is not commercially available anymore.

Work such as ``Development of a Miniature Robot for Swarm Robotic Application" updates the processor at the heart of a robot similar to Alice, called AmIR, but there is no evidence that AmIR was ever widely available\cite{arvin2009development}. Similarly, the robot described in ``A Miniature Mobile Robot With a Color Stereo Camera System for Swarm Robotics Research'' combines a relatively modern microprocessor with a DSP for on-board vision processing \cite{haverinen2005miniature}. Again, it is not a platform that other researchers could buy.

The Jasmine swarm robots were possibly the closest thing to a successor to Alice.
Jasmine measured 26x26x20mm, and included an ATMega processor, IR close range communication and obstacle detection, two motor skid steering, and li-po batteries.
Jasmine V was intended as of 2007 to include aggregation/modular robot functionality.  Unfortunately, Jasmine units cost about 100 Euro each when they were available. 
Plans and information to reproduce Jasmine are available, but the chassis of Jasmine is a custom mechanical assembly, rather than a commercially available product. 
This work intends to demonstrate that modified toys are an adequate substitute for custom mechanical assemblies, and permit easy experimentation with heterogeneous swarms. 

The Epuck from EFPL is approximately ~800 swiss francs per unit, so the cost of maintaining a large swarm can become daunting quickly. Michael Bonani's MarXbot runs into a similar problem, in that it has a strong computer and a rich set of sensors and effectors, but as a result it is quite expensive \cite{bonani2010marxbot}.

Clearly, it cannot be expected of all swarm robotics researchers that they start and maintain a side business supporting and selling robots (much as the author might appreciate it).
However, in the interest of allowing easy reproduction of research work, and extension of swarm platforms, it might be wished that all of the information required to reproduce swarm hardware platforms be made available in an easily-reproduced form.
One collateral goal of the work outlined in this paper is to produce a well-documented reference design that can be implemented by a researcher using common tools, and possessed of no more than hobby-level familiarity with hardware, and common tools. 
If researchers are not expected to become hardware entrepreneurs, they should also not be expected to become expert machine tool operators. 

One robot that is both low in cost and commercially available is the Kilobot. \cite{rubenstein2014kilobot}. Kilobots contain about \$15 worth of parts, but a 10-pack sells for 1100 Swiss francs, or about \$112 per robot. This is certainly more competitive than e.g. the Epuck. 

However, the Kilobots do not have hardware ehterogeneity beyond that produced by wear on the robots over time. They also move by stick-slip motion, and so must operate on a smooth surface. Kilobots also do not permit highly heterogeneous programs. For very small groups, individual robots can be programmed differently, but any attempt to give each of a very large collection of robots an unique program will take a long time. By combining wireless connectivity and unique identifiers, the robots described in this work can be programmed without having to handle each robot. 

%synthesis of other projects
The previously described projects tend to fall into one of two groups, from a hardware perspective. The first group uses microcontrollers and very limited onboard computation, but is small and relatively cheap. Due to their limited computation, these systems do not support complex algorithms such as vision processing. 
All of these control systems that involve a toy plus some control hardware don't really go into how the system deals with running more advanced algorithms on the robots (because they can't, the hardware doesn't support it), or with making and operating a large cluster of them (100 \$500 laptops on RC cars is still expensive). The software of my system is the interesting part, as it allows partitioning the power of a single large computer over a lot of ``virtual'' smaller computers, while still having electromechanical systems in the real world, and all the detail that that brings.

%my improvements

``Occlusion-Based Cooperative Transport with a Swarm of Miniature Mobile Robots" Jianing Chen et al. If you can't see the goal because the object is blocking your view, push on the object. 

%$\Rightarrow$ What if there isn't a command language that makes sense for a 1-2 dozen robot swarm?
\subsection{Swarm Robot Software}

The basic software infrastructure consists of the software executing on the robot, modules on the control computer that are emulated as running on the robot, and support software. 

Because children's toys are manufactured to fairly low tolerances, the correlation between motion commands issued by the control software and resulting motions of the robot are prone to error. 
The robots are also intended to be heterogeneous, partly because of the advantages of heterogeneity in a swarm, and partly because toy supplies are unreliable.
While toys in the general case are expected to remain available, a particular line of toys might be discontinued or a modified version released. 
The system as a whole should be robust against the addition of new types of mobility hardware. 
Because of the selection of hardware with a WiFi module, every hardware module is programmed with a unique MAC address. 
In order to learn the control rules, the control software will send motion commands to each robot, and observe the displacement of that robot. 
By varying motion commands and observing the resulting displacement, the system will learn the effect of motion commands on each robot, and so be able to control the robots. 
Observation of the magnitude of displacement of the robots for similar motion commands, the system will also be able to quantify swarm members based on speed. 
Variation of response to motion command will allow the system to be able to rank individual robots by the reliability of their motion. 
As a result, the system as a whole can develop measures of the abilities of the robots, which may influence the later selection of swarm members for action assignment. 

Because the system has an omniscient-view camera to track the robots, other objects in the robot arena can also be tracked. 
For example, obstacles can be created by drawing lines on the floor of the robot enclosure. 
Different colors could represent different types of obstacles, or qualities of the obstacles that are relevant to the software under test. 

Networking between the robots will be handled by the central system, but emulating a different network topology. 
From the point of view of the robots, messages sent into the central system will be delivered to other robots as if the messages were sent directly from one robot to another. 
By changing how the messages are delivered by the central system, we can implement full connectivity, range-limited mesh networking, directional beacons, or other forms of networking. 
We can also vary the reliability of the network, by dropping some messages or reducing parameters based on elements of the virtual environments. 
For example, signals that pass through a virtual wall may have a reduced emulated RSSI and range, or may not arrive. 

This virtual networking can be extended to other virtual robot properties, such as emulated sensors, and controlled to emulate error conditions such as reduced speed, depleted batteries, noisy sensors, degraded localization, and so forth.
Virtual parameter tweaking will allow fine-grained testing of the behavior of algorithms under imperfect conditions, and the response of human users to unreliability in the swarm. 

\subsection{UI Designs}

The current state of the art in design for a decentralized swarm controller appears to be one of two methods. (TODO define why I'm not doing centralized control)
The first method is to come up with behavior primitives that can be performed as a function of local sensing by each member of the swarm. 
These primitives are then composed to come up with a program that the programmer thinks is likely to result in the desired behavior. 
The program is then tested and modified based on the result of the testing in an iterative fashion to converge on the desired behavior. 
Genetic algorithms and similar approaches have been explored for automating the iterative development of composed programs. 

Neither of these approaches describes communication back to the user. 
Programming languages and frameworks can be considered an HRI of sorts, but they are distinct from the HRI of the software as it runs. 

One approach to getting feedback from a swarm was the development of the Swarmish sound and light system\cite{mclurkin2006speaking}. 
Autonomous charging means the robots can have long runtimes, and minimal one-on-one interaction with humans, so the interactions are all remote, and to a certain extent "ambient". 
An ambient interaction is one where the information is continuously available, and the human user ``tunes in'' to it when needed. Swarmish uses a set of colored lights, with a total of 108 different combinations of colors and blink sequences, as a visual indicator. 
In addition to the lights, each robot can produce MIDI notes over its audio system. 
Each note can vary in instrument, pitch, duration, and volume, in addition to having tempos and rhythms as the code executes. 
The designers of Swarmish indicate that the sum of the audio output of the swarm can provide a overall idea of the status of the swarm, but that as a musical instrument, it is difficult to play well. 
Further, the use of lights as signaling mechanisms assumes that you can look at the robots. 

Assuming that the robots are visible to the user, the robots can carry some form of display that provides local information to the user. 
This information can then be displayed as an overlay in the real world, with the display of the information conterminous with its presence\cite{Daily:2003:WEI:820752.821587}. 
Local display of local information works if the user is part of a hybrid human/robot team, and so is in the same location as the users. 
%``World Embedded Interfaces for HRI". \cite{Daily:2003:WEI:820752.821587} 
% For very large numbers of robots, anything that depends on unique identities for each robot won't scale. 
%Centralizing control or representing data won't translate easily into something humans can deal with. 
%Neat "Jar full of little robots" image. 
%Local information as gradients, overlaid on real-world with e.g. augmented reality. %Robots as pixels, active fiducials (e.g. bokodes). 
%Directional beacons on robots so they can see which way a message came in (have seen this before in other papers). 
%This is only an interface for output, does not send commands to the robots. 
%Does raise the distinction between hybrid teams where the person is in the environment with the robots and robot-only teams where the person is elsewhere. 

%``Speaking Swarmish: HRI Design for Large Swarms of Autonomous Mobile Robots" \cite{mclurkin2006speaking} How to maintain, program, interact w/ robots without doing it on an individual robot basis. 
%Autonomous charging is an enabler (my swarm doesn't have this...). 
%Lights and sounds of robots as output channel. 
%Again with the robot-as-pixel (which assumes you can look at them). 
%Remote programming is key, they do it by having something remarkably like bittorrent across all the robots, updating in parallel. 
%Something using TCP or UDP broadcast would be good. 
%GUI "Inspired by real-time strategy video games". 
%Colored LEDs on top of robots (~12 single-light patterns, several variations, 108 total patterns), audio system. 
%Audio system plays MIDI, single notes; instrument, pitch, duration, volume are variable. 
%Tempo and rhythm variation as code executes. Requires considerable user effort to ``play'' well. 

%$\Rightarrow$ Look into real-time strategy games 

For situations where the user is not located in the same area as the robots, one possible approach is a ``call center'', where robots can request human attention when required \cite{chen2011supervisory}. The human in the call center, however, is faced with having to answer potentially multiple calls with no awareness of the robot's situation. Operator multitasking starts to fail once there are more than about 16 robots being operated at once, so scaling to kilo-robot swarms seems impractical.  

Supervisory control has the human act as the planner and monitor of the systems being supervised, but allowing the systems to operate on their own.
Automation is frequently broken down into ten levels of automation, with 10 being a fully automatic system with no humans involved, and level 1 having no automation, such as a bicycle \cite{parasuraman2000model}. 
It would be expected that reducing the number of times the human is required to interact with the robot will permit the user to operate more robots. 
Level 5 is a sort of operation by consent model, where the computer chooses a route and executes it if the human permits it. 
Level 9 is the inverse of level 5, where the computer informs the human only in exceptional cases. 
In situations with even moderate numbers of robots, level five may generate too many events for the human to deal with. 
As the number of robots increases further, even level nine may overwhelm the operator. 
However, increasing the use of automation may also create new difficulties by leaving operator out of practice, or encouraging mis-placed trust in the automation's ability. 

The use of models such as developing a grounding using a proxy for the robot may assist the user in redeveloping SA \cite{stubbs2008using}. 


Olsen \& Wood Fan-Out for UGVs around 5-9, depending on environment complexity. Again, bad for swarms. UAV and UGV together have reference frame issues (exo- vs. egocentric) when trying to combine incoming data to present to user. 

Automation behavior should be continuously visible and explainable to user. 
User should be able to extract meaning from the information display quickly, as in the case of Swarmish and the robot-as-pixel desgins.
As the system changes, the changes and predictions should be highlighted so that the user understands consequences of their actions. 
Re-instruction of the system should be quick \& easy, although this may be problematic for a system that operates on an automated cycle of planning, coding, compiling, and running, because programming generally is not fast.
% ``Interfaces... are efficient when they are compact and when they accentuate information that guides the decision making process" So part of making the interface good is making it good at deciding what to tell the user and what to hide. 
%Multimodal (visual/audio, visual/haptic)interfaces are faster and result in perceived reduction in workload. Scheduling incoming information that the user has to deal with reduces workload bottlenecks. Indicating which robots are in a team with a semi-transparent form covering the team was slightly preferred by user to individual robots, in interfaces where all robots are depicted (need to cite this). 

One possible approach to maintain a constant and managable workload on the user is adapting the level of automation to the workload. 
When the load is low, the user is more directly engaged, but when load is high, there is more automated assistance. 
Adaptation does not have to be based on measured load, but could instead be based on perceived load or physiological markers in the user. 

%TeamTalk, TraderBot, and Plays - Task allocation and shared mental model work for human/robot teams. May also work for conveying the swarm model to the user so that they understand what is going on. $\Rightarrow$ Unsure if it can convey commands or queries back, but probably can. 

\subsection{User Interface Design}

Ecological interface design presents a possible guide to the architecture of user interfaces for swarm robotics \cite{vicente1992ecological}. 
User ability to interact with a system is separated into a taxonomy of skills, rules, and knowledge. 
The user has skills, which are rote, simple activities that form the basis of the normal operation of the system. 
Rules allow the user to handle exceptions or unusual cases that have come up before. 
Rules do not require the user to understand the system, just to know that when certain situations are recognized, certain other actions must be performed in response. 
Knowledge allows the user to handle novel exceptions. 
The user has an understanding of how the system works, and can apply that knowledge to react to situations that they have not experienced or been told about before. 
Events are also broken into three levels: routine, which uses skills; foreseen exceptions, which use rules; and unforeseen exceptions, which use knowledge. 
All levels should be supported by the interface, but the user should not be forced to operate at a higher level than is required. 
In order to control a process, the human/machine system must account for the complexity of the process and the constraints of the work domain. 
The abstraction of the process maps onto the hierarchy of ecological design, with the highest level being the function of the process and the lowest level being how the function is accomplished. 
Detection of exceptions requires the display of all constraints on the process, because exception is the breaking of constraints, and undisplayed constraints cannot be assessed to determine if they hold.

Once interaction some section of the process has been taken over by automation, the user operates primarily in the rules and knowledge areas, dealing with exceptions \cite{vicente2002ecological}.
The interface should allow direct manipulation of perceptual forms that map directly onto work-domain constraints and represent all of the information identified by the abstraction hierarchy. 
In a swarm context, this means displaying functional information in such a way that the user can move across the hierarchy from individual swarm bots to high-level swarm-wide tasks, and interact at all levels to control the swarm. 
EID is well-positioned to deal with emergent behavior, because the emergent behavior of the entire system is present at the functional level, but is composed of actions at the physical level.  
%"Reciprocity of user and environment, represenative design of experiments and evaluations, primacy of perception, and start with analyzing the environment". Information about a system is abstracted across a hierarchy, high level is functional information ("This plant makes reciprocating flange whompers"), low level is physical information ("This is a fleeble grommet, it is located in the engine room"). Hierarchy is over work domain, not task. EID should encourage skill and rule-based behavior (for normal operations) while also allowing knowledge-based behavior (to solve unanticipated problems). 

Previous work in multi-touch interfaces directly relates to EID by providing both an omniscient camera view for direct manipulation of the high-level, functional actions of the entire swarm, and the ability to move down the hierarchy to control individual swarm members \cite{Micire:2009:ANG:1731903.1731912}.
Multitouch interfaces have been determined to improve on WIMP or voice interfaces for multi-robot control in a sequence of command and control tasks, including commanding the swarm to a location, performing reconnaissance, and having the swarm cross a dangerous area \cite{hayes2010multi}.
The interface displayed the locations of the robots on a directly manipulatable map, and used included movable or semi-transparent user interface widgets, in order to minimize occlusion of the map. 
Areas were selected with with drawing gestures, and paths with fluid strokes, rather than e.g. selection of vertices bounding an area.
%Faster task completion, especially for region selection. ``This work was partially supported by a contract from the US Marine Corps Systems Command to M2 Technologies, Inc." Basically, a lot like stuff our lab has found out, (hurf durf, because they cite Mark/Drury/Yanco on multi-touch-multi-robot). NASA TLX for human factors like frustration and workload. 
\subsection{Metrics for the Behavior of Swarms}

BRING TRUST WORK IN 

In order to determine the quality of the behavior of the swarm, its efficacy at performing tasks must be measured. 
Harriot et al propose that metrics for measuring the interaction of humans and swarms differs significantly from the interaction of humans and individual robots \cite{harriott2014biologically}.
Nine classes of metrics are proposed. 
The most obvious differences from individual robot control metrics are the ideas of leadership within the swarm, and macromovement of the swarm. 
Human attributes - how the human interacts, trust, intervention frequency, etc. \\
Task performance - ability to accomplish task, speed, accuracy, cost \\
Timing - Command diffusion lag, behavior convergence \\
Status - Condition of the swarm, battery life, number of functioning members, stragglers \\
Leadership - Interaction between special members of the swarm and others, how well leaders are followed \\
Decisions - How actions are taken, likelyhood that the correct action is chosen \\
Communication - Speed, range, network efficiency \\
Micromovements - Motion of individual swarm elements relative to each other \\
Macromovement - Motion of the overall swarm, flocking, elongation, forming fancy shapes \\ 
Harriet et al also put the estimated transition point between multi-agent control and swarm around 50 individuals. 
Above that threshold, human interaction may be able to remain focused on macro level behavior, influencing the overall behavior of the swarm rather than control of individuals. 

Swarms have more uncertainty, because the reliability of individual robots is low; and higher attentional demands because there are many robots. 
It may be that above some threshold, the attentional demand will drop again, as the group is no longer treated as a large number of individuals, but as a single group. 
The user interface may be able to drive this re-imagining, and convey other information, by depicting the group in different ways \cite{manning2015heuristic}.
The base case is to simply display all the units as individuals. 
Other approaches include an amorphous shape covering the area occupied by the swarm, an amorphous shape with density shading and motion arrows, the fields of influence for leaders in the swarm, and the web generated by the flow of information within the swarm. 
Considered as a whole, the swarm has properties, such as center of gravity or flock thickness, that do not exist in individual robots. 
Views of these properties may assist the user, for example in determining what areas have insufficient robot density for a thorough search operation. 
%A lot of Dr. Adams' work in HSI was under ONR Award N00014-12-1-0987

If the user is unconcerned with the functioning of individual swarm members, so long as the swarm as a whole remains functional, the UI may simply drop malfunctioning individuals from view. 
This handling of error conditions on individual swarm units fits with the assumption that the swarm as a whole achieves robustness through redundant expendable units, while also allowing the human user to have a rough idea of how the situation is developing by watching the cloud shrink. 
Do long as progress appears to be being made on the mission, the user might let underperforming units slide. 
The supervisory system might not even announce when units are lost, until it starts to affect performance.  
In the limit, the swarm could be treated as a gas, and for tasks such as diffusion over an area, the performance of the swarm would be compared to the behavior of an ideal gas \cite{jantz1997kinetics}.
The addition of sensors and computation would then allow the robots to outperform a gas at tasks, and so achieve higher scores on a task-oriented metric than a gas could attain. 
 
%$\Rightarrow$ Is trust even a factor with a swarm? Trust of swarm as a whole vs. individual.
%$\Rightarrow$ What level is a swarm expected to operate at? At the swarm-as-a-whole level, it's more or less level 1, human tells swarm what to do. At the individual robot level, it's 7 or higher. 
%$\Rightarrow$ Is it a valid assumption that the swarm is never teleoperated? What is the use case for being a single ant or bee, especially out of thousands of units?
%$\Rightarrow$ Look into MiDAS (Mission Displays for Autonomous Systems). 

\subsection{Derive a gesture language}

\cite{Kato:2009:MIC:1520340.1520500} has a multi-touch top-down interface, user specifies vector field. Just intended for moving robots around. Focus on the vector field rather than individual robots, so this could be handy for e.g. gathering and dispersal. Makes assumptions (for display) about knowing all the locations of all of the robots that might not be available in a real situation. On the other hand, the exact locations of the robots don't need to be known if the display is e.g. cloud of robots. Vector field paths can't cross each other (they flow together), but the can have loops (which waypoints can't do, as they have explicit ends) None of this results in a program that works on the robot without communication, as the computer orders all the robots around based on a global knowledge (the field).


Do user study to determine if there is a control language for multi-touch interfaces for robots.
Method similar to Mark's work, but for way more robots than the user has fingers. 
Dozens to hundreds of robots. 
Same tasks or similar tasks? Same, but possibly with some extensions. 
High level tasks like searching an area, finding something. 

Determine if grammar exists. 

Is there a non-multitouch way to do this? Beyond a few robots, one-at-a-time selection doesn't scale. Need mass selection, maybe spreading from a point while the pointer is active?

Will users care about unused or underused robots? At what point, count-wise, does the user stop caring about individuals and start ordering the robots around as a cloud or flock? Does framing the question to the user in a specific way allow us to vary that parameter, so that by couching our questions in a certain way, we can decide what the user cares about and how they command the robots?

$\Rightarrow$ What about shaping and pushing the swarm the way kids will play with a bug, putting their hand down so the bug goes around or avoids it, touching the back of the bug gently to make it scurry forwards, etc? Shaping the group as if sculpting, with pushing and pinching to carry groups around?

Could have selection based on space-partitioning, so there is no way to leave a robot out because of the shape of a partition. E.g. split space in half, tap splits tapped half, etc. Select a region and go. 

If grammar exists, implement in simulation for e.g. multitouch

Implement for real robots (the little tanks, the swarm I'm making, Harvard kilobots)

If no grammar exists, why not?
How would we detect failure to converge to a grammar? No two users using the same commands, or very poor inter-user cohesion in the command sets. If no gestures are more likely than any others, then there is no preferred command set or gesture set. 
This is actually not much of a setback, as it lets me select a grammar and develop it; or select pair of grammars, and then compare them.

One possibility is to define a command grammar like the programming environment Tierra. 
In Tierra, there is no such thing as an invalid program. 
All sequences of the existent symbols are regarded as executable programs, although some are more functional than others. 
While this does prevent the possibility of the user becoming frustrated by being told that their commands are invalid, it runs into other problems. 
Good user interface designs permit easy undoing of an undesired command, but with the physical world, this can be difficult. 
It is much simpler to undo a digital representation of a paint stroke in a illustration program than it is to remove paint from a real wall, and it is far easier reset a simulation of a robot than it is to recall a real robot from falling down the stairs. 
Furthermore, a command language with no invalid commands runs into the same problem as gesture interfaces: there is no way for a user to be idly present to the interface.
Every signal is interpreted as something that must be acted upon. 
%Allow users to operate swarm over internet, see what people try to use to control a swarm
%Ends up causing issues because most users don't get to try (one user at a time) if it is commanding a real swarm. Swarms in simulation or web-based interactive pages don't have that problem. Can also record all the input. 
%Can't possibly be multi-touch, as user hardware and OS support isn't really there. Most people won't do it if your instructions start with ``get a second mouse''. 
%Could be multi-user with real robots or simulation, but runs into problems with arbitrating between which commands are used. Could do sum-of-vectors or something for shared control, like haptic/shared autonomy stuff. Could also just let users partition the robots by negotiation among themselves. Not sure how communincation for this would work. 
\subsection{Conversion of User Tasks into Robot Programs}

"Programming the swarm" \cite{evans2000programming} David Evans (Looks like a doctoral thesis proposal) Swarm as collection of mobile, physically situated processors communicating over an ad-hoc network. Behavior of swarm as emergent from behavior of units, resilient against "misbehaving members". References Amorphous Computing. Two approaches to programming: compose program from primitives, or synthesize unit programs based on description of environment and desired behavior. Analysis is approximate, rather than general proof of properties.

Compositional approach strikes me as something like abstracted Tierra, where any combination of behaviors is a valid program, but might not be a good/useful program. 

User interface allows the user to define an overall desired distribution of the robots, position and directions, and then the control software compiles that into a representation in e.g. GPL or OSL that gets compiled into programs to run on the robots. This isn't a continuous controller, more fire \& forget. Could have robots with a main behavior of successfully navigating the world and other behaviors (search and rescue, track target, etc.) as riders. Sort of like putting sensors on existing bees, and then collecting the data while they do bee stuff. 

A lot of the applications listed in PtS are similar to foraging search. Spread into area, locate and converge on objective. Paper is mostly on lightweight formal methods and policy checking. Could be used at a policy level for a swarm, e.g. to limit overall power use, but adds another layer of behavior that can alter emergency. Lists as primitives: Disperse (no other nodes within distance d), general disperse (no more than n nodes within distance d), clump/cluster, attract to location, swarm in a direction, scan area, Broadcast/partialcast/unicast. Composition of programs as serial, parallel, divided, global (synchronized). I think the only composition that makes much sense is the one observed in biological organisms, that is, priority based parallel evaluation, where different behaviors become active as context requires. 

Mobile software specs: "Mobile UNITY [Roman97], the Distributed Join-Calculus [Fournet96], and Mobile Ambients [Cardelli98]."

"A much more ambitious goal is to produce the device programs for a primitive swarm program from the high-level behavior description. It is not realistic to hope that we could completely automate the process." This is pretty much what we're talking about doing. 

Lists tuple spaces, amorphous computing, as potential enabling technologies for swarm programming. 

"Continuous Space-Time Semantics Allow Adaptive Program Execution" \cite{bachrach2007continuous} Jonathan Bachrach, Jacob Beal, Takeshi Fujiwara. Spatial computer model, devices that interact depending on their location in space. Computer is "programm[ed]...as a continuious space". Swarm robotics as a special case of the spatial computer. "Proto" is language for a continuous plane spatial computer. Operations act as reduce() or similar over local area or history, feedback loops provide state. Paper provides emergent computation of firefly-style synchronization (coupled oscillators, so perhaps some elements of swarm computing can be expressed as variably-coupled oscillators in combination with driving signals derived from internal states and sensor inputs. This would in some ways be the ultimate evolution of the BEAM "Robot Jurassic Park" idea from the '90s, where each robot is sharing some of its internal state via coupled oscillation with the other robots in the park, e.g. to communicate the presence of light sources for ``food", the presence of self for collision avoidance, etc.)

"The mechanism for binding sensors to names is implementation dependent, as are the value when sense is applied to an unbound name and the result of multiple streams being sent to the same actuator." So, uh, the actual interaction of the program with the world is ``implementation dependent"? Good, knowing what your code does is for suckers. 

"Programming with Stigmergy: Using Swarms for Construction" \cite{mason2003programming} Zachary Mason. Homogeneous swarm with no memory, only local perception. A set of environmental triggers is "coherent if no stage in the building process can be confused with an earlier stage by making only local observations, thus obviating the need for centralized control". Agents move at random, actions governed by perception of environment at present time. Environment can include pheromones.  "Other future work includes programming construction swarms by specifying the target structure directly, letting a compiler infer the corresponding rule-set (if one exists)."

"A Formal Approach to Autonomic Systems Programming: The SCEL Language" \cite{nicola2014formal} ROCCO DE NICOLA et al. Self-managing autonomous ensembles of systems. Only related to swarm robotics in the sense that it is for more than one computer. Assumes complete localization and inter-swarm communication. 

"Scripting the Swarm: Event-based control of Microcontroller-based robots" \cite{magnenat2008scripting} Stephane Magenenat et al, EFPL. Introduces ASEBA, an event-based control architecture. Reflashing e-puck takes a minute, doesn't scale to a bunch of robots unless you can do it to all robots at once. Concurrent debugging is impossible. Programs compiled to bytecode, react to incoming sensor data (e.g. message from other robot, impending collision, etc.). Could compose swarm behavior by deciding reaction priorities, tuning priorities in reaction to environment. Still doesn't describe task-to-program compilation. Does permit differing code between robots, which won't scale to 1k+ robots. 

"Structure Synthesis on-the-fly in a Modular Robot" \cite{revzen2011structure} Shai Revzen et al. Robot uses binary foam to make structures, including bodies for other robots by sticking foam to robot parts. Not really a swarm thing, unless you count the other robot parts.  

"Automated Construction using Co-operating Biomemetic Robots" \cite{bowyer2000automated} Adrian Bower. Robots that build as an emergent property of individual behavior. Has a few basic rules to govern arch and column building. Mostly an exploration of potential ideas, without a lot of conclusions. Isocyanurate foam is awful stuff. 

"Collective Construction with Multiple Robots" \cite{wawerla2002collective} Jens Wawerla, Gawav S. Sukhatme, Maja J. Mataric. Behavior based, virtual sensing. Communication and memory, even minimal, will improve the efficency of the system, but are not required to complete the task. Task is implied by the behaviors, not compiled from a higher-level specification. 

"Emergence-Oriented Programming" \cite{palmer2005emergence} Daniel W. Palmer, Marc Kirschenbaum, Linda Seiter. Mentions GA to produce agent programs, could probably be used for developing behavior priority vectors. Hard to determine success, need to be able to recognize and evaluate performance on sub-problems. "The ideal way to program a swarm is to simply shout out high-level objectives and let the swarm figure out how to solve the problem." Meta-rules that govern selection of rules at the individual agent level depending on agent context. Nested hierarchies of systems and their contexts, but can be collapsed to an arbitrary level and its next level up (higher has no downward effect, lower can be considered part of base level). All information for the actor is local (its own sensors and memory, can possibly share with other actors). Uses aspect-oriented-programming (AoP) to recognize method invocations for single-agent actions as an aspect (e.g. maintain-distance and move-random forming a dispersion aspect). Analysis of the behavior of the swarm looking for good patterns and trying to develop code that does not yet exist to create behaviors that have not yet been observed is REALLY HARD, so have humans do it. This is a design process, not an automation process, but indicates a direction for possible ML approaches (which seems like a return to the GA approaches the authors dismissed earlier, possibly as less-directed than human intervention.). This requires multiple design/build/test loops for each activity the swarm tries to do. Perhaps have it learn forageForObject until object is found, surroundObject until object is surrounded, then "pushObject" until object is where it should be, then disperseRobots until the object is left alone again. Humans can do ``what-if" experiments on individual robots (so can the computer, and faster, assuming some evaluation function is available). Swarm algorithims extracted from Virtual Human Swarm (VHS) having people wander around a gymnasium. 

What is the existing work in compilation of programs for robots? 

Are there swarm-specific languages? 

"Proto" is a language for a continuous plane spatial computer. Proto paper also lists TinyDB as a "database view of the devices making up the computer", *LISP, and Regiment. Continuous time evolution approximated in gamma calculus and P-systems. 

"Infrastructure for Engineered Emergence on Sensor/Actuator Networks" \cite{beal2006infrastructure} Devices share internal state with local neighborhood, all run the same code bu can diverge due to neighborhood influences, sensor values. Proto is a stream-processing language, program is a directed acyclic graph with a single root (the output stream), composing programs is placing nodes into the graph. Proto is strongly typed, but with type inference rather than explicit/static typing. Doesn't cover conversion of a task into code. 

$\Rightarrow$ Look into *LISP, Regiment, gamma calculus, P-systems, Functional Reactive Programming, Gooze

"Combining Amorphous Computing and reactive agent-based systems: a paradigm for pervasive intelligence?" David Servat, Alexis Drogoul. Eventually everything will have an embedded, networked processor. Everything will continue to be gnarly: high heterogeneity in terms of software, processors, use cases, controllers and users. Things should still behave in a useful, coherent, predictable way. Requiring users to constantly manage them is not that. Amorphous computational model: computational units, which may be faulty, randomly located in a dynamic environment. Homogeneous programming, limited communication radius. Paper holds that most research in the area is computational models (e.g. ant colony optimization) or communications methods. Hop-count based gradient methods, bio-inspired based on quorum sensing. ``AC Hierarchy" for performing computing tasks ``simplifies programming with high-level abstractions". 

"Software Eng. For Self-Adaptive Systems" (Is a 270 page book, maybe skim later?)

"A Catalog of Biologically-inspired Primitives for Engineering Self-Organization" \cite{nagpal2004catalog} Radhika Nagpal.\\ 
"1. Morphogen gradients and positional information \\
2. Chemotaxis and directional information \\
3. Local inhibition and local competition \\
4. Lateral inhibition and spacing \\
5. Local monitoring \\
6. Quorum sensing and counting: \\
7. Checkpoints and consensus \\
8. Random exploration and selective stabilization:"\\
First five are common in amorphous computing, last three not so much (what are their uses?). Calls out gradient sensing in neuronal morphogenesis. Local inhibition and competition for symmetry-breaking. Quorum sensing to detect agent count required for a task (``do we have enough robots to move this box?") JIT allocation rather than pre-allocation is more robust, use the robots that are in the right place at the right time. Domino timing, where completion of each phase triggers the next. 

"Co-fields: Towards a Unifying Approach to the Engineering of Swarm Intelligent Systems" \cite{mamei2003co} Marco Mamei1, Franco Zambonelli. Again, the problem of robust software in dynamic environments with unreliable actors. Computational Fields (co-fields) are distributed data structures in a space, abstracting some features of the world perceived by the agents. Available as a dynamic systems formalism. Agents move on gradients in field spread by agents into the environment (great shades of pheromones, Batman!). Space doesn't have to be physical space. 

"Engineering Amorphous Computing Systems" \cite{nagpal2004engineering} Radhika Nagpal, Marco Mamei. Intro very similar to "A Catalog of Biologically-inspired Primitives for Engineering Self-Organization". Programming languages for AC: Growing Point Language (GPL, thanks, we needed some ambiguity) differentiates to form structures. Origami Shape Language, for dense agents on a deformable sheet. Describes local folds to eventually form an origami pattern. Both compile local program from overall description of desired result. Languages have different global properties. TOTA middleware provides fields and propagation rules for data, similar to co-fields. 

"Notes on Amorphous Computing" Jacob Katzenelson. \cite{katzenelson2000notes} Doing differentiation and integration on amorphous computers. 

$\Rightarrow$ Why is almost everyone ignoring power supplies except as an afterthought? Not a computer science problem?

$\Rightarrow$ Is it useful to share sensor information in a broadband rate-coded way, e.g. spike trains? Neuronal computation seems a bit like the limit case of amorphous computing, except that the network isn't radius-bound. 

$\Rightarrow$ Are there languages to describe a problem that is to be handled by a swarm, and compile to code for swarm members?

Assume the robots have a set of behaviors available (approach other robots, send a certain signal, move, avoid other robots, ``emit pheromone'') and can conditionally execute them e.g. when they see another robot, or when they reach a location. Is it possible to convert from user tasks to priority sequence or if-this-then-that style ``programs''?

Could be argued to be ``composition" from primitives.

``A Compositional Framework for Programming Stochastically Interacting Robots" \cite{napp2011compositional} Nils Napp \& Eric Klavins. Guarded Command Programming with Rates (GCPR). Robot behavior modeled as a stochastic process. Swarms are inherently concurrent (paper calls this the primary challenge, which I kind of doubt). Concerned with rigorous proof of correctness, which I maybe should be. ``Composed programs execute simultaneously". Assumption is that robots only have local sensing. GCPR guards are conditions on the environment, when condition is met, robot does actions at a given rate. This sounds a LOT like behavior-based robotics. In the concurrent case, this is modeled as each action happening one at a time, but in random order. Swarm behaves in a state space S, want to ensure that for all orderings of all actions, S ends in target final state. Physicality of system limits state transitions (robots can't occupy same space, can only move fixed distance in given time, etc.). Programs can be sped up or slowed down by scaling. Rate is typically the inverse of the mean time to complete the action (i.e. do things as fast as you can get them done). Markov semantics of the system description means robots randomly choose when to perform actions, once guards are met. By scaling time of actions, relative influence of actions is increased or decreased (e.g. if no obstacles move right with scale 4, move left with scale 2 means robots will move left or right, but twice as fast to the right, so robots will drift right overall). Failed behaviors are modeled as a program that doesn't do what the other program it is composed with does. Correct programs are those that reach the target state with probability one, even when composed with bounded failures (note that they might do it very slowly, but once they succeed, they stay successful).

$\Rightarrow$ In the real-world case of noisy or imperfect sensors, the variable time to execution of a guarded behavior would be caused by the imperfection of the robot's ability to detect that the guard was satisfied. In the real world, we get stochasticity for free. 

$\Rightarrow$ Get Klavins et al 2k4, apparently on algorithmic approach mapping local to complex global behavior.  

"Evolving Self-Organizing Behaviors for a Swarm-Bot" \cite{dorigo2004evolving} MARCO DORIGO AND VITO TRIANNI et al. The whole front page is authors. Swarm-bots assumed to be able to connect to each other, so this is a modular approach. Formation of an aggregate and motion of that aggregate. Evolution is often very slow, especially in physical systems, so simulator used. Evolve in simulation, test in hardware. Genotype is the weights of a single-layer perceptron that connects the sensors to the wheels. Evolved motion strategy figured out that spinning in place was the best move, which isn't what the users wanted. Aggregation evolves well, motion ends up being slow, and slowing as robot count increases beyond evolved values. A lot of the algorithims appeared to stablize (fitness-wise) after 25 or so generations. For mutual motion when linked, robots could sense traction, allows negotiation of a common direction of movement by proprioception. Can avoid obstacles and pull objects due to traction interactions. 

Didn't have enough robots to test in real life. Some decrease in performance when switching to a more accurate physical model for simulations. 

$\Rightarrow$ Quinn et all 2001, 2003, apparently did work on individuals (aclonal), rather than all robots having the same genome (clonal) Quinn, M. 2001a. A comparison of approaches to the evolution of homogeneous multi-robot teams. In Proceedings of the 2001 Congress on Evolutionary Computation (CEC2001), IEEE Press: Piscataway, NJ, pp. 128–135.
Quinn, M. 2001b. Evolving communication without dedicated communication channels. In Proceedings of the Sixth European Conference on Artificial Life, J. Kelemen and P. Sosik (Eds.), vol. 2159 of Lecture Notes in Computer Science, Springer-Verlag: Berlin, Germany, pp. 357–366.
Quinn, M., Smith, L., Mayley, G., and Husbands, P. 2003. Evolving controllers for a homogeneous system of physical robots: Structured cooperation with minimal sensors. Philosophical Transactions of the Royal Society of London, Series A: Mathematical, Physical and Engineering Sciences, 361:2321– 2344.

"Evolving Aggrgation Behaviors for Swarm Robotic Systems: A Systematic Case Study" \cite{bahgecci2005evolving} Erkin Bahceci and Erol Sahin. From Mataric, evolution is only useful if it's faster than designing it by hand. Aggregation as a preliminary behavior, before doing something of interest (moving object, attacking en mass, etc. ). 

"Behavioral Feedback as a Catalyst for Emergence in Multi-Agent Systems" \cite{palmer2005behavioral} Daniel W. Palmer, Marc Kirschenbaum, LindaM. Seiter, Jason Shifflet, Peter Kovacina (2005). Not a lot of structure in swarm programming, lots of ad-hoccery, no automated agent program generation. Agents respond to emergent behavior in a sort of loop, or split-level hierarchy where the emergent behavior is the upper level and the agent behavior is the lower level. Basic swarm agents are assumed to be reactionary. Next level up uses local storage. Again with the AoP, behavioral feedback as an aspect. Used stochastic selection of actions, so kine of like Napp/Klavins work. 

What are the sorts of problems that can be solved by hardware-heterogeneous, code-homogeneous robots? what about het-het, homo-het, homo-homo? Does the distribution of hetero- or homogeneity affect the sort of problems that can be solved? Does it affect solution time or complexity? From a biomimetic standpoint, this seems very similar to asking if worker ants learn. If they learn, then they become hardware-homogeneous and software-heterogeneous, and can at least solve ant problems. If they don't, then they are homo-homo, and can still solve ant problems. Any problem that divides the robots into groups may be regarded as software heterogeneous, but could also be regarded as running different parts of the same ``if-else'' structure. 

A concept of the swarm as a whole as a programmable entity runs into trouble with reliability. In conventional compilation, assuming the compiler is correct and the computer is correct, the compiled binary does what the source code says. Robots interact with the real world, which is much less likely to be ``correct'' in the same sense a compiler can be asserted to be. Programs for swarms are only going to be functional within some probabilistic grounds and assumed conditions. 

How much of the behavior of the robots can be determined at compile time, and how much has to be determined at runtime?

Ideally, the robots can perform the task without requesting recompilation of the software running on each robot. 
This requirement indicates that the situation has to be at least somewhat known ahead of time, so that the robots will all receive programs that allow them to perform the task.
In the ideal case, the emergent action of all of the robots interacting with the environment causes them to perform the task. 

How can a user task get broken down into programs for individual robots?
Is it possible to leave room for emergency to solve problems in novel ways, or will everything have to be specified in predictable ways?

How are programs or behaviors for robots constructed? 
As a subset of this, how does varying percentage of a population that is running a certain program affect the emergent behavior of the system as a whole? 

How are individual robot actions parameterized? 
E.g. for clustering, range that robots can see each other, likelihood of a robot deciding to be a cluster center.  
Can variable parameterization be effective in the face of heterogeneous robot behavior?

Is it possible to build a compiler that converts user commands in task space into programs for robots without having to code in assumptions about the qualities of the robots? Does the compiler have to account for global properties (e.g. GPS denial)? Can the compiler account for robot-local properties (e.g. battery life, motor failure)?

How is task completion detected? Bio-inspiration would probably be something like quorum sensing or other local-communication-based saturation methods. Once all the robots are saying the task is locally completed, then the task is completed for at least the coverage area. 

``Stupid Robot Tricks: A Behavior-Based Distributed Algorithm Library for Programming Swarms of Robots", \cite{mclurkin2004stupid} James D. McLurkin. Set of comm techniques and library of behaviors for multiple robots in a group. Gradient-flood communications. Swarm behavior emergent from individual behaviors is sometimes unexpected. Library provides parts that interact predictably. Tested on 100 real robots. Was advised by Brooks, so there's the behavior-based robotics mentioned in the title. "Insects to not seem to have global names..." Neither do people (Eric, Eric...). Algorithimic state stored in the physical world, e.g. with stigmurgy or robot positions. Robots may not all be able to communicate with each other, network can be divided by distance, paper assumes they are all in one connected component (no way to know if they are not without known global IDs "is bob here?"). Robots might have global, local, or no IDs. Transmission of signals between robots is CSMA, can have collisions, limits throughput. Different types of gradient messages, with different propagation rules and rates. 

Behaviors ideally run concurrently, some respond to sensor inputs. IN the paper, the behaviors output is whether they are running, translational and rotational velocity, and LED configuration. Subsumption and summation are used to arbitrate between behaviors of differing priorities. 

Motion $\rightarrow$ moveArc, moveStop, moveForward, moveByRemoteControl, bumpMove

Orientation $\rightarrow$ orientForOrbit, orbitRobot, orientToRobot, matchHeadingToRobot, followRobot

Navigation $\rightarrow$ followTheLeader, orbitGroup, navigateGradient

Clustering $\rightarrow$ clusterOnSource, clusterWithBreadCrumbs, clusterIntoGroups

Dispersion $\rightarrow$ avoidRobot, avoidManyRobots, disperseFromSource, disperseFromLeaves, disperseUniformly

Utility $\rightarrow$ detectEdges

Demos ended up calling multiples of these. Motion specifications were (for individual robots) things like degrees of bearing and centimeters of proximity, so behaviors can be quantified. detectEdges is to detect if a robot is on the edge of the network. Behavior-based robotics explicitly doesn't learn, so ``bad environments" can defeat the swarm. Unfortunately, there's no such thing as a ``bad environment", just environments. Paper doesn't provide axioms for swarm programming at the group level. 

$\Rightarrow$ iRobot swarm had directional IR signaling (quadrants) using signal strength to figure out range and bearing, so that's something I'll want to include or emulate in my system. 

$\Rightarrow$ I should probably include some form of charging that doesn't involve a lot of hands-on interaction with my robots. 

``Pheremone Robotics" \cite{payton2001pheromone} Virtual pheremones with directional sensors. Largely a rehash of "World Embedded Interfaces". Modulated IR for sending messages. Hop-counting for diffusion. Budding with growth inhibition for exploration. Gas model as well, attraction balancing repulsion. Cites interesting work in directed diffusion, amorphous computing (processing with reaction-diffusion systems). "No explicit maps or models of env, no explicit knowledge of robot location". Walking in the world rather than walking in your head. 

``Compound Behaviors in Pheromone Robotics" \cite{payton2003compound} Compilation from this paper's ``set of logical primitives for controlling the flow of virtual pheromone messages" from some higher-level task representation may be useful. Virtual pheromones are local (no global information), diffusion-graded (spatial information), and decay over time (old information goes away). Can also contain other data, rather than just presence/absence. Paper describes set of primitives, composes gradient follow, go hide, and cooperative sensing out of those. Inhibition out of something very like quorum sensing. Building of commands still done by hand. 

$\Rightarrow$ Research quorum sensing in bacteria.

$\Rightarrow$ Could steal from neuronal wiring and growth for exploration and communication algorithms. 

% research questions (usually, 1-3 should suffice) and the reason for asking them
% the major approach(es) you will take (conceptual, theoretical, empirical and normative, as appropriate) and rationale
% significance of the research (in academic and, if appropriate, other fields)

%Highlight its originality and/or significance
%Explain how it adds to, develops (or challenges) existing literature in the field
%Persuade potential supervisors and/or funders of the importance of the work, and why you are the right person to undertake it

Computer is aware of meaning of gesture, locations of robots. 
Second assumption (global localization) may not hold. 
How can computer determine what programs or what parameters result in the completion of the task?

What are the tasks? Assume search and rescue domain, what are the tasks in USR? Search an area with good coverage. Report content of an area. Group at a location. Locate a specific resource. 

This could be situated at an intersection between planning and compiling, as the compilation might have to factor in elements of the known environment at the time of compilation. Since the actors are spatially situated, plans should incorporate spatial awareness.  

``Improving Efficiency in Mobile Robot Task Planning Through World Abstraction'' \cite{galindo2004improving} Hierarchical world representation and operating at multiple levels of that hierarchy to reduce dimensionality of the problem space. Successive refinement from broad goals to behavioral primitives. Primitives are available for swarm robotics, but overall actions take place over the swarm. How to actually accomplish the translation is a bit tricky. Even GPL and OSL work from a desired known final state to a behavior specification, so task specification would have to be based on a description of the final state of the system. 

Robots have some autonomy, similar to the existing work with extending to multi-actor teams, e.g. human/robot mixed team. 

Task allocation if robots are heterogeneous (battery life, terrain, etc. will make homogeneous into heterogeneous)

Proximity if robots are not (closest robot does task)

Difficulty arises when robots are not super-well localized

``Swarm robotics: a review from the swarm engineering perspective" \cite{brambilla2013swarm} Manuele Brambilla, Eliseo Ferrante, Mauro Birattari, Marco Dorigo. (2013) ``Swarm engineering is an emerging discipline that aims at defining systematic and well founded procedures for modeling, designing, realizing, verifying, validating, operating, and maintaining a swarm robotics system." Two taxonomies, one of design/analysis methods, one of collective behaviors. Classes automatic design of robot programs as either evolutionary or multi-robot reinforcement, no category for program synthesis at all. Trial-and-error approaches are a different area. Behavior-based with probabilistic FSMs, virtual physics (gas models and potential fields). Other stuff category: amorphous computation. 

Models tend to be macro or micro, not a lot of modeling across all levels of the swarm behavior. Paper has overviews of some of the mathematical models, not super-relevant to designing the program, but probably handy for simulation. Use of real robots rates a section, less than half of papers actually use real robots. People don't often state why the real robots were used (real sensor noise, real failures, things simulation elides). 

Collective behavior hierarchy covers a lot of the behavior primitives that papers on compositional models cover. 

HSI section just lists a bunch of interactions, gesture-based communication, robots observing operator and voting/consensing on what they operator signaled. Human placement of beacons or interaction via a central computer. 

$\Rightarrow$ Protoswarm language! There's a language for that! Find a paper, do it now! Unfortunately, this has UI of a programming language, so still not great for first responders who are not also coders. 


\subsection{GPS-denied Swarm Control}

The Global Positioning System (GPS) has provided machines with a sense that humans lack: relatively precise localization within the global scale, in a format that is easy for computers to detect.
Using GPS makes it easy to localize an individual member of the swarm, and so coordinate its actions relative to other similarly-localized swarm members. 
However, GPS is less than ideal for swarm coordination for several reasons. 

In the most basic case, GPS signal may not be present. 
The GPS signal is broadcast from solar-powered satellites, and so is a very low-power signal.
At the earth's surface, it is so weak that it can be blocked by buildings or dense tree canopies, meaning that there are both indoor and outdoor areas with poor or no GPS reception \cite{zheng2005quantitative}. 
Even if a GPS signal is present, it could be incorrect. 
Because the GPS signal is a weak signal on a known frequency, a malicious actor can jam or spoof the signal with a minimal investment in hardware and power\cite{montgomery2009receiver}.

In order to work around these problems with the GPS signal, some location-aware devices, such as cell phones, also base their position on the received signal strength of cell phone towers and wifi access points. 
However, in both urban search and rescue and wilderness search and rescue contexts, either of these services may be unavailable. 
USAR may be motivated by a wide-ranging disaster, such as a hurricane, which removes electrical power, and with it most wifi and cellular infrastructure.
Because urban disaster areas by definition include buildings, at least partial GPS denial is a valid assumption.  
Wilderness areas are unpopulated, and so have no demand to drive the placement of wireless communications. 
Adding additional radios and processing to take advantage of these other wireless localization options will also increase the size, cost, and energy consumption of the swarm robots.

Global positioning, however, is not the only kind of positioning available to swarm members. 
Indeed, there is no evidence that biological swarms, be they schools of fish, flocks of birds, or swarms of insects, have any idea about their global position. 
It has been demonstrated that local processing and local perception, even without communication, can generate complex multi-actor behaviors such as flocking, surrounding an object, dispersing in a directed fashion, or pushing an object towards a beacon \cite{mclurkin2004stupid, chen2015occlusion, reynolds1987flocks}.

These sorts of behaviors are preferable to centrally-coordinated action (of which GPS is an example), because they are robust to uncertainty in the performance of the communication links between the actors. 
If communication between a central controller and the robots fails, the robots become uncoordinated. 
If communication between a robot and the other robots fails, that robot may be lost, but the overall actions of the swarm may continue relatively unimpeded.  
This provides robustness to the robot swarm because robots can replace each other. 
However, this robustness is meaningless if the control software cannot adapt to loss of individual robots, or generates plans that are not robust in the face of failures of individual robots. 
A chile playing with blocks can use a blue block if there's not a red one available, or two small ones if there's not a large one, and software that claims to support a robust robot swarm should be at least that flexible.

%Determine how denial of a few usually-assumed-good properties of the system affects how the user tasks get compiled. 
%Degrade and then remove inter-robot communication to see how system adapts when robots can't reach each other. 
%Degrade and then remove global localization (GPS denial) to see what happens when robots cannot determine global absolute distance each other.

The distances and locations available without a global positioning system are usually relative positions. 
Robots can find the distance to each other by direct measurement using sensors, or by measuring the received signal strength (RSSI) of incoming communications from other robots.
Because such measurements are not ``attached'' to any fixed points in the world, it may be possible for the robots to form a grid, but impossible to orient that grid to point north. 

With a shared map, robots may leverage their communication with each other to more quickly converge on their own location. 
For example, SLAM with AMCL localization can generate spurious location hypotheses when the robot enters an area of the amp that is similar to another area. However, if another robot is present and aware of it's own location, it may provide a distinction between the two points by informing the other robot that if it is at one of the hypothetical locations, they should be able to see each other. 

Unreliable positioning can lead to unreliable behavior. 
If robots in a chain are required to maintain a certain distance from each other, variable noise in the distance sensing will result in the chain becoming unstable as individual links are erroneously perceived as being too close or too far apart. 

Have to use e.g. foraging search to find each other, do area coverage, update map. Want to avoid redundancy in search, but that's hard to do without either marking the environment (collect whatever it is they are searching for, so later seekers detect that it is scarce and move on quickly, leave a repellent pheromone) or global mapping (mark areas of map as searched, move to unmapped areas). With high enough robot count and known area, diffusion/scattering will get a certain robot/area density that could count as ``coverage''. 

Ant colony optimization or similar approaches to swarming on a point (forming chains away from the point to collect others, following chains)

In a real application environment, stigmurgy is somewhat less applicable than it is in simulations. 
In simulation, robots that use pheromones to communicate have unlimited reservoirs of pheromones and flawless perception of the pheromones emitted by other robots. 
In reality, the difficulties of limited resources and flawed perception decrease the efficiency of stigmurgic communication. 
For example, in the USAR efforts after hurricane Katrina human search and rescue workers spraypainted markers on buildings to indicate the results of building searches to each other. 
Computer vision has not yet reached a point where reading similar markers would be reliable. 
Beyond technical concerns, a robot which leaves marks on its environment may not be desirable to the humans it will have to operate around. 
Most parents train their children not to scribble on walls, and would prefer their robots are at least equally well-behaved. 

How to convey to user/world map that maybe their robots are not where they think they are (for all possible disambiguations of "they"). Circles around last-seen locations? Becomes counterintuitive to control something you can't see or determine if it's doing the right thing. How do robots develop their concept of doing the right thing, and how do they convey that along with uncertainty about it to the user?


\bibliography{swarm.bib}
\bibliographystyle{plain}

\end{document}

%Papers to read

%Cooperative interaction of walking human and distributed robot maintaining stability of swarm 

%Development of IR-based short-range communication techniques for swarm robot applications 

%The Wanda Robot and Its Development System for Swarm Algorithms

%Stability of swarm robot based on local forces of local swarms

%Swarm robot pattern formation using a morphogenetic multi-cellular based self-organizing algorithm 

%A particle-swarm-optimized fuzzy-neural network for voice-controlled robot systems 

%The I-SWARM project 


