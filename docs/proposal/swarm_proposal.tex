\documentclass[]{article}

\usepackage{xargs} 
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}

%opening
\title{Command Language for Single-User, Multi-Robot Search}
\author{Abraham Shultz}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

Methods for command and control that are based on issuing individual orders to individual actors, whether they are robotic or human, do not scale to large numbers of actors. 
By defining a mapping from user interface gestures to individual programs loaded on local robots, we can allow an individual to control arbitrarily large, heterogeneous swarms.
In order to remain robust in the face of failure, the overall action of the swarm should be decentralized emergent behavior, rather than a centralized orchestration. 
Each robot receives its own program, and the sum of the execution of the programs on each robot results in task completion as emergent property.

\section{Research Questions}

In order to create a mapping from a command language to a set of individual robot programs for multi-robot command and control, the command language must first be specified. 
The command language in this work is a system of common gestures selected using an empirical process as in \cite{Micire:2009:ANG:1731903.1731912}. 
%Is there a coherent command language for controlling large swarms?  
For the purposes of this research, an intuitive command language is one that is freely chosen by the majority of users. 
In \cite{Micire:2009:ANG:1731903.1731912}, for each available command, one or two gestures were used by 60\% of the users. 
The use of multi-touch interaction is desirable because one-at-a-time selection doesn't scale beyond a very limited number of robots.
In order to interact with large groups of robots, the user must be able to perform operations on areas and groupings, rather than on the single point available with a traditional pointer-based interface. 
Multitouch interfaces have been determined to improve on WIMP or voice interfaces for multi-robot control in a sequence of command and control tasks, including commanding the swarm to a location, performing reconnaissance, and having the swarm cross a dangerous area \cite{hayes2010multi}.
The interface displayed the locations of the robots on a directly manipulatable map, and used included movable or semi-transparent user interface widgets, in order to minimize occlusion of the map. 
Areas were selected with with drawing gestures, and paths with fluid strokes, rather than e.g. selection of vertices bounding an area.

It is possible that there is no intuitive command for some task. 
If this is the case there will be little commonality of gestures used between users for that task.
If no two users use the same command the same commands, or, more generally, there is very poor inter-user cohesion in the command sets, then there is not an intuitive command for the tasks.
This is not the severe failure it may appear to be, as even without a consistent command grammar, it is possible to compare selected command gestures for usability. 

Because the focus of this work is to determine how gesture control scales or fails to scale, and humans primary mode of gesturing is with their hands, the size of the swarm should be significantly larger than the number of fingers a user could gesture with. 
In order to have a less tongue-in-cheek definition of ``large'', the scale required for a swarm to be considered ``large'' will be determined empirically.
It is expected that there exists a transition point for the number of members in a swarm where users will stop interacting with the UI representation of the members of the swarm as individuals, and attempt to interact with the representations as groups or collections. 
For example, rather than selecting each robot by clicking on it, the may ``paint'' over the area containing the robots they want to use. 
A large swarm is, then, a swarm with a number of members above the point at which such a transition occurs. 

The transition point may be detected in a number of ways. 
It is hypothesized that above the transition point, users will be more likely to neglect some subset of the available robots. 
The user will instead use commands that control the bulk of the robots the robots as a cloud or flock, but may leave some robots unused. 
For example, the user may switch from selecting robots as individuals to shaping and pushing the swarm the way a child might play with a bug, putting their hand down so the bug goes around or avoids it, touching the back of the bug gently to make it scurry forwards, and so forth, or by shaping the group as if sculpting, with pushing and pinching to carry groups around. 

It may also be the case that framing the statement of the task to the user in a specific way will allow some control over the interaction method the user uses. 
Changing how information on the swarm is presented to the user may also affect the user's choice of interaction method. 
 
Once the language is defined, sequences of commands in that language must be translated into programs to execute on each member of the swarm. 
The program for each swarm member must be generated automatically, either by creation of a new program or by a synthetic approach from per-determined behavior primitives.
Because the motivating examples used in swarm robotic research are almost never related to swarm software development, the end user cannot be expected to do the programming themselves. 
All of the valid expressions possible in the command language should be converted into programs for the robots, or the user must be usefully informed as to why it was not possible. 
The synthesized program should result in convergence of the swarm's overall behavior to the desired result. 
Clearly, in a developing situation in the real world, success may be impossible, and so there is not a practical way to guarantee that a particular valid command sequence will result in a particular desired state of the world. 
However, certain minimum bounds on the problem may be able to be used to determine if a desired task is certain to fail.
% research questions (usually, 1-3 should suffice) and the reason for asking them
% the major approach(es) you will take (conceptual, theoretical, empirical and normative, as appropriate) and rationale
% significance of the research (in academic and, if appropriate, other fields)

%Highlight its originality and/or significance
%Explain how it adds to, develops (or challenges) existing literature in the field
%%Persuade potential supervisors and/or funders of the importance of the work, and why you are the right person to undertake it

\section{Work Plan}

\subsection{Build a swarm}

In order to experiment with swarm robotics, a swarm must be made.
Much can be done in simulation, but it is a daunting prospect to simulate the dynamics of real robot motion and the changes in the performance of each swarm member as mechanical wear and other forces influence them. 
Further, swarm simulations must not have bugs in the software which results in incorrect behavior. 
Genetic algorithms are infamous for exploiting quirks of their simulation environment which are not present in the real world, but still allow members of the population to inflate their fitness. 
Any behavior of a physical, unsimulated swarm is the real behavior of the swarm, and cannot be blamed on a defect in the simulation of the swarm or its environment. 

\subsection{Swarm Robot Hardware}

Swarm robots are generally small. 
The reason to keep swarm robots small is two-fold. 
First, larger robots consume more materials per unit, and so costs more money.
As a result, for a given number of swarm units, larger robots will result in a higher cost swarm. 
Second, each robot requires some amount of space to move around in. 
To keep the ratio of free space to robots constant, the area of space used by the robots grows as the robots do. 
If the ratio isn't kept constant, the robots will crowd each other, and so large robots will require either a very large space, or become overly crowded.

\subsubsection{Overview of Previous Swarm Hardware}

The use of COTS hardware in research robotics has lead to at least two platforms refered to as COTSBots.
Bergbreiter's COTSBots used mote hardware for the communications link and sensing, plus a motor control add-on board  \cite{bergbreiter2003cotsbots}. 
The mobility platform is a hacked toy, in particular, a specific brand of high-quality micro RC car.
At the time of this writing, the particular car used is moderately expensive for a toy car, although quite cheap for a research robot, costing a over \$100USD per unit. 
Bergbreiter's COTSBots use TinyOS, a modular and event-driven framework for developing node software. 
ROS also provides a modular, event-driven framework, so many of the design ideas, if not the code, are equally applicable to the system described in this paper. 
TinyOS is written in a dialect of C called nesC rather than ROS's polyglot approach. The motor and mote boards communicate using a messaging layer, again like ROS. 
The motor driver board is not commercially available, but can be custom-built by board fabrication companies, without the researcher having to assemble it by hand. 
Ohio State also developed a very small microwave RADAR that can go on the boards.

The second version of COTSBots arrived 8 years later, in ``COTSBots: Computationally Powerful, Low-Cost Robots for Computer Science Curriculums" \cite{soule2011cotsbots}. Soule and Heckendorne describe a platform composed of a laptop, which controls a modified RC car, tank, or similar toy through some combination of motor drivers. 
Due to the diversity of possible combinations of hardware that can be assembled into this configuration, it is still a very viable platform. 
However, the minimum size of this style of COTSBot is the size of a laptop, which is in turn dictated by the minimum size of a useful keyboard. 
The large size of these COTSBots demands a very large space if the density of robots in a large swarm is to be kept low. 
Additionally, each laptop has a screen, keyboard, and so forth that are not useful while the robot is operating. 
All of these parts add to the overall cost of the swarm. 

Many impressive designs for swarm robot platforms have been proposed, and constructed, but are no longer easily commercially available, or never were. 

At the low end, in terms of scale, the I-SWARM Project intended to create a 2x2x1mm robot that moved by stick-slip locomotion actuated by piezo levers\cite{seyfried2005swarm}. 
Over the course of the project from 2004-2008, the hardware was developed and used in research, but was not converted to a commercial product. 

Alice, by Caprari et al. packed a PIC16F84 processor, motors, RF and IR networking, and enough battery power for 10 hours of autonomy into a robot measuring under one cubic inch \cite{caprari1998autonomous}. 
Unfortunately, the processor is anemic by modern standards, and the platform as a whole is not commercially available anymore. 
Several similar projects are described in the literature, but they are also not commercially available. 

Work such as ``Development of a Miniature Robot for Swarm Robotic Application" updates the processor at the heart of a robot similar to Alice, called AmIR, but there is no evidence that AmIR was ever widely available\cite{arvin2009development}.
Similarly, the robot described in ``A Miniature Mobile Robot With a Color Stereo Camera System for Swarm Robotics Research'' combines a relatively modern microprocessor with a DSP for on-board vision processing \cite{haverinen2005miniature}. 
Again, it is not a platform that other researchers could buy.

The Jasmine swarm robots were possibly the closest thing to a successor to Alice.
Jasmine measured 26x26x20mm, and included an ATMega processor, IR close range communication and obstacle detection, two motor skid steering, and li-po batteries.
Jasmine V was intended, as of 2007, to include aggregation/modular robot functionality.  

Unfortunately, Jasmine units cost about 100 Euro each when they were available, and they are no longer available for purchase. 
The plans and information required to reproduce Jasmine units are available, but the chassis of Jasmine is a custom mechanical assembly, rather than a commercially available product. 
This work intends to demonstrate that modified toys are an adequate substitute for custom mechanical assemblies, and permit easy experimentation with heterogeneous swarms. 

The Epuck from EFPL is approximately ~800 swiss francs per unit, so the cost of maintaining a large swarm can become daunting quickly. Michael Bonani's MarXbot runs into a similar problem, in that it has a strong computer and a rich set of sensors and effectors, but as a result it is quite expensive \cite{bonani2010marxbot}. 

%synthesis of other projects
The previously described projects tend to fall into one of two groups, from a hardware perspective. 
The first group uses microcontrollers and very limited onboard computation, but is small and relatively cheap.
This group includes Alice, Jasmine, AmIR, and so forth.
Due to their limited computation, these systems do not support complex algorithms such as vision processing. 
The second group use more powerful computers, but at a significant cost in weight, power consumption, and financial outlay.
By developing a system of inexpensive hardware and software that permits the hardware to behave as though it were under the control of a more powerful computer, my system will bridge that divide. 
The power of a single large computer will be partitioned over a large number of ``virtual'' smaller computers, while still having electromechanical systems in the real world, and so avoiding the pitfalls of simulation. 

Clearly, it cannot be expected of all swarm robotics researchers that they start and maintain a side business supporting and selling robots (much as the author might appreciate it).
However, in the interest of allowing easy reproduction of research work, and extension of swarm platforms, it might be wished that all of the information required to reproduce swarm hardware platforms be made available in an easily-reproduced form.
One collateral goal of the work outlined in this paper is to produce a well-documented reference design that can be implemented by a researcher using common tools, and possessed of no more than hobby-level familiarity with hardware, and common tools. 
If researchers are not expected to become hardware entrepreneurs, they should also not be expected to become expert machine tool operators.

\subsubsection{Proposed Swarm Hardware Design}

The challenge of construction of swarm robots hardware, then, is to put all of the same parts as non-swarm mobile robots: a mobility platform, a processor, some sensors, and a communication system, into a small package.  
The robots described in this work accomplish this task by using a Commercial Off-The-Shelf (COTS) module to provide Wi-Fi connectivity and a microcontroller for processing. 
The module used in this swarm hardware is a ESP8266-03  \unsure{Get a picture of the ESP8266}, which provides a Wi-Fi interface and approximately 500kB of flash memory for programs. 
The ESP8266 is avialable in several form factors, each designated by a different suffix. 
The 03 version was chosen because it offers more GPIO pins than most other versions, and includes an internal antenna, which some versions lack. 

This swarm controller module was designed to be used as a replacement for the control electronics of children's toys, similar to the \unsure{Cite Ionas} swarm robots developed in PAPER. 
Most children's toys use either one motor with a mechanical linkage to cause the toy to turn when the motor is reversed, or two motors.
Two-motor toys frequently use either differential steering or have one motor provide drive power and the other provide steering. 

In order to be both heterogeneous and inexpensive, the robots used for this work will be constructed by developing a consistent control hardware platform that can be attached to children's toys. 
Each controller will use a ESP-8266 wifi module for wireless communication and as a micro controller. 
The ESP-8266 costs approximately \$3-5, and contains both a wireless interface and a micro controller that can be programmed from a variety of programming environments and languages, including Lua and the Arduino variant of C/C++. 

In order to locate the robots within the experiment area, an overhead camera is used to detect QR codes on each robot. 
The QR codes provide location, heading, and a unique identifier for each robot. 
In this system, the central computer provides virtualization of the processing on each robot. 
Virtualizing the computational resources on each robot effectively allows us to borrow robots from the future. 
As technology develops, higher powered computers and and more capable sensors have become cheaper and smaller. 
Allowing each swarm node to have a virtual computer ``riding'' on it gives us as much computational power as we want at each point in the swarm, even if hardware of that power level is not currently available.
The robots themselves are effectively Internet of Things (IoT) nodes with the ability to move around. 
This is, then, an extension of the work done with mobile sensor motes called COTSBots \cite{bergbreiter2003cotsbots}.

Currently existing swarm robots are too expensive to build a large swarm, with the exception of the Harvard Kilobots \cite{rubenstein2014kilobot}. 
Kilobots contain about \$15 worth of parts, but a 10-pack sells for 1100 Swiss francs, or about \$112 (US) per robot. 
However, the Kilobots do not have hardware heterogeneity beyond that produced by wear on the robots over time. 
Wear and tear on the robots will result in differences between the robots, even if they had started from identical conditions. 
Even sending part of a swarm up a hill while the rest remain at the bottom will result in the higher members of the swarm having less battery life than those that didn't move. 
Battery life may be relevant to allocation of tasks, meaning that the physical elevation gradient of the robots becomes a gradient of fitness for a task as well. 
For very small groups, individual kilobots can be programmed differently, but any attempt to give each of a very large collection of robots an unique program will take a long time. 
The kilobots also move by stick-slip motion, and so must operate on a smooth surface, such as a whiteboard. 

However, heterogeneity can also be leveraged to increase the robustness of a swarm and reduce its cost. 
A hetergeneous swarm containing both flying and ground robots can allow specialization of the individual components. 
By way of historical analogy, airplanes and cars have been separate types of vehicles, each specialized for a particular task. 
Flying cars, however, are normally either poor cars, poor planes, or both, because the demands of each task are different. 
Cars require a certain amount of weight and downward force for traction, but weight and downward air forces are the enemy of developing a good airplane. 
Rather than requiring that all robots be able to traverse all terrains, the ground robots can be developed to cover ground well, while the aerial robots are optimized for longer ranges or dwell times. 
Heterogeneity also allows the robots to be cheaper, because not every robot has to do everything.
Combining robots that can fly and robots that can roll allows a mixture of overview sensing and mission endurance that would be difficult to produce robustly and inexpensively with robots that can do both. 

The decision to use ESP-8266 wifi modules as the core of the swarm robots designed in this work is an extension of a long legacy of the use of off-the-shelf components in previous robotics projects. 

\subsection{Swarm Robot Software}

The basic software infrastructure consists of the software executing on the robot, modules on the control computer that are emulated as running on the robot, and support software. 

Unfortunately, such small processors do not have significant computational power. 
The majority of the processing will be performed on a host computer running the ROS software framework. 
However, the host computer will not be a central control node for the swarm. 
Instead, the host computer will include a separate process for each robot in the swarm, which will be allowed to drive only the robot that it is linked to. 
Each of these robot processes will effectively act as a local control program for the robot, but will have the full processing resources of the host computer. 
As a result, the individual robots can be small, lightweight, and relatively low power, but the system as a whole will endow them with significant computing power. 

Software for the swarm, then, will be of two classes. 
The first class is infrastructure software that provides things like a medium for communication between the robots, or emulating proximity sensing for each robot. 
Having communication mediated by software on the host will allow for simple experimentation with variable network bandwidth or reliability. 
The second class of software is the program that controls each robot. 
The system as a whole will not constrain the robot control programs to be identical across all robots, but it may be useful to constrain the robot control programs to be the same for certain experiments. 

Because children's toys are manufactured to fairly low tolerances, the correlation between motion commands issued by the control software and resulting motions of the robot are prone to error. 
The robots are also intended to be heterogeneous, partly because of the advantages of heterogeneity in a swarm, and partly because toy supplies are unreliable.
While toys in the general case are expected to remain available, a particular line of toys might be discontinued or a modified version released. 
The system as a whole should be robust against the addition of new types of mobility hardware. 
Because of the selection of hardware with a WiFi module, every hardware module is programmed with a unique MAC address. 
In order to learn the control rules, the control software will send motion commands to each robot, and observe the displacement of that robot. 
By varying motion commands and observing the resulting displacement, the system will learn the effect of motion commands on each robot, and so be able to control the robots. 
Observation of the magnitude of displacement of the robots for similar motion commands, the system will also be able to quantify swarm members based on speed. 
Variation of response to motion command will allow the system to be able to rank individual robots by the reliability of their motion. 
As a result, the system as a whole can develop measures of the abilities of the robots, which may influence the later selection of swarm members for action assignment. 

Because the system has an omniscient-view camera to track the robots, other objects in the robot arena can also be tracked. 
For example, obstacles can be created by drawing lines on the floor of the robot enclosure. 
Different colors could represent different types of obstacles, or qualities of the obstacles that are relevant to the software under test. 

Networking between the robots will be handled by the central system, but emulating a different network topology. 
From the point of view of the robots, messages sent into the central system will be delivered to other robots as if the messages were sent directly from one robot to another. 
By changing how the messages are delivered by the central system, we can implement full connectivity, range-limited mesh networking, directional beacons, or other forms of networking. 
We can also vary the reliability of the network, by dropping some messages or reducing parameters based on elements of the virtual environments. 
For example, signals that pass through a virtual wall may have a reduced emulated RSSI and range, or may not arrive. 

This virtual networking can be extended to other virtual robot properties, such as emulated sensors, and controlled to emulate error conditions such as reduced speed, depleted batteries, noisy sensors, degraded localization, and so forth.
Virtual parameter tweaking will allow fine-grained testing of the behavior of algorithms under imperfect conditions, and the response of human users to unreliability in the swarm. 

\subsubsection{UI Designs}

The current state of the art in design of software for a decentralized swarm controller appears to be one of two methods. \unsure{define why I'm not doing centralized control}
The first method is to come up with behavior primitives that can be performed as a function of local sensing by each member of the swarm. 
These primitives are then composed to come up with a program that the programmer thinks is likely to result in the desired behavior. 
The program is then tested and modified based on the result of the testing in an iterative fashion to converge on the desired behavior. 
Genetic algorithms and similar approaches have been explored for automating the iterative development of composed programs. 
The second method is to define roles for each robot, and a sequence of actions to be performed by each role. 
\unsure{Citations needed, is this as brittle as it sounds? This is where we get markets for selection of roles.}
Neither of these approaches describes communication back to the user, and while programming languages and frameworks can be considered an HRI of sorts, neither of these approaches describes the HRI of the software as it runs. 

One approach to getting feedback from a swarm was the development of the Swarmish sound and light system\cite{mclurkin2006speaking}. 
Swarmish provides an ambient means of determining the overall state of the swarm, as well as some information about individual robots. 
The swarm that used Swarmish had autonomous charging, and so the individual robots had long runtimes, and minimal one-on-one interaction with humans. 
As a result, most of the interactions were remote.
The ``ambient'' aspect of the interaction is that the information is continuously available, and the human user ``tunes in'' to it when needed. 
Swarmish uses a set of colored lights and sounds, produced by each robot, to provide feedback. 
The lights were in three colors, and had a total of 108 different combinations of colors and blink sequences, as a visual indicator of the state of each robot. 
In addition to the lights, each robot could produce MIDI notes over its audio system. 
Each note can vary in instrument, pitch, duration, and volume, in addition to having tempos and rhythms as the code executes. 
The designers of Swarmish indicate that the sum of the audio output of the swarm could provide a overall idea of the status of the swarm, but that as a musical instrument, it is difficult to play well. 
Further, the use of lights as signaling mechanisms assumes that you can look at the robots. 

If we accept the assumption that the robots are visible to the user, the robots can carry some form of display that provides local information to the user. 
This information can then be displayed as an overlay in the real world, with the display of the information conterminous with its presence\cite{Daily:2003:WEI:820752.821587}. 
Local display of local information works if the user is part of a hybrid human/robot team, and so is in the same location as the users. 

However, there are many situations where the robot is not in the same location as the user. 
A common example is urban search and rescue, where buildings may be known to be unsafe, or of unknown stability, but it is desirable to search them for trapped people. 
In such a situation, the human user would rather be located elsewhere, and receive information from the robots. 

For situations where the user is not located in the same area as the robots, one possible approach is a ``call center'', where robots can request human attention when required \cite{chen2011supervisory}. The human in the call center, however, is faced with having to answer potentially multiple calls with no awareness of the robot's situation. 

One method that is frequently cited as a basis for call center UI is Supervisory Control. 
Supervisory control has the human act as the planner and monitor of the systems being supervised, but allowing the systems to operate on their own.
Automation is frequently broken down into ten levels of automation, with 10 being a fully automatic system with no humans involved, and level 1 having no automation, such as a bicycle \cite{parasuraman2000model}. 
It would be expected that reducing the number of times the human is required to interact with the robot will permit the user to operate more robots.
At level 1, the user has to interact constantly, and so could not be expected to operate more than one robot. 
By increasing the level of autonomy of the robot, the time required for the user to operate the robot decreases.  
Level 5 is a sort of operation by consent model, where the computer chooses a route and executes it if the human permits it. 
At level 5, the robot only contacts the user to confirm plans, and does not occupy the human's time while the plans are in progress. 
Level 9 is the inverse of level 5, where the computer informs the human only in exceptional cases. 
One possible approach to maintain a constant and manageable workload on the user is adapting the level of automation to the workload. 
When the load is low, the user is more directly engaged, but when load is high, there is more automated assistance. 
Adaptation does not have to be based on measured load, but could instead be based on perceived load or physiological markers in the user. 

However, in situations with even moderate numbers of robots, it may be that even relatively high levels of automation may overwhelm the user. 
Level 5 is a fairly high level, but with a large number of robots checking in, even this level may generate too many events for the human to deal with. 
As the number of robots increases further, even level nine may overwhelm the operator, despite the fact that the robots are only checking in with the operator when an exceptional situation occurs. 
Increasing the use of automation may also create new difficulties by leaving operator out of practice, or encouraging mis-placed trust in the automation's ability. 

In fact, the any kind of multitasking may prove insufficient for large swarms. 
Operator multitasking starts to fail once there are more than about 12 robots being operated at once, in the best cases.
Failure generally takes the form of task effectiveness no longer increasing as more robots are added.
Instead, the amount of time the user spends interacting with the robots begins to outweigh the time that the robots are usefully active between interactions, and so the time the robots spend uselessly waiting for interactions rises. 
Most of the best cases are uncrewed aerial vehicles (UAVs), which require relatively little oversight. 
Uncrewed ground vehicles (UGVs) require more oversight than UAVs, due to the higher complexity of the ground environment. 
Estimates place the limits on the number of robots under control at 12 or 13 for UAVs and 3-9 for UGVs \cite{WangSearchScale}.  
There is some latitude, at least in UGVs, to increase multitasking by increasing automation, as shown by the relatively wide range in the interaction limits, but even 9 is nowhere near the scale of kilo-robot swarms \cite{Olsen:2004:FMH:985692.985722}.
%The use of models such as developing a grounding using a proxy for the robot may assist the user in redeveloping SA \cite{stubbs2008using}. 

Another possible approach to a control UI for a remotely-located swarm is a multi-touch interface for specifying a vector field \cite{Kato:2009:MIC:1520340.1520500}.
This interface does not allow the assignment of tasks to robots, but allows the user to directly control the motion of the robots. 
Because the user interface design focuses on the vector field rather than individual robots, the same control interface can scale to an arbitrarily large collection of robots. Vector field paths can have loops, which do not exist in waypoint-based paths. 
Waypoint paths have explicit ends, unless an additional command is added to join beginning and ending points. 
Vector field paths do have some limitations, however. 
Because the vectors are bound to a 2-D plane, the paths they create cannot cross each other. 
Instead, they flow together. 
This user interface does directly map to programs on the robots. 
Instead, the central computer maintains the vector field representation and commands the individual robots.

The use of co-fields may provide a way to move the vector field representation from the central computer to the swarm, or allow the swarm to act for some time without constant updates from a central controller \cite{mamei2003co}.
Co-fields distribute the data within a space, which may be physical or may be abstract. 
Agents react to gradients in field, and spread their own fields over local communication networks. 
The overall vector space created by the user (the UI vector space) could be propagated to the robots periodically, and combined with their own internal vector fields to generate movement based on both the user's desires and the local rules operating on each robot. 
However, knowing which areas of the UI vector space are relevant to each robot may require  global localization, and so only be available for swarms operating in conditions that permit global localization. 

Ecological interface design presents a possible guide to the architecture of user interfaces for swarm robotics \cite{vicente1992ecological}. 
User ability to interact with a system is separated into a taxonomy of skills, rules, and knowledge. 
The user has skills, which are rote, simple activities that form the basis of the normal operation of the system. 
Rules allow the user to handle exceptions or unusual cases that have come up before. 
Rules do not require the user to understand the system, just to know that when certain situations are recognized, certain other actions must be performed in response. 
Knowledge allows the user to handle novel exceptions. 
The user has an understanding of how the system works, and can apply that knowledge to react to situations that they have not experienced or been told about before. 
Events are also broken into three levels: routine, which uses skills; foreseen exceptions, which use rules; and unforeseen exceptions, which use knowledge. 
All levels should be supported by the interface, but the user should not be forced to operate at a higher level than is required. 
In order to control a process, the human/machine system must account for the complexity of the process and the constraints of the work domain. 
The abstraction of the process maps onto the hierarchy of ecological design, with the highest level being the function of the process and the lowest level being how the function is accomplished. 
Detection of exceptions requires the display of all constraints on the process, because exception is the breaking of constraints, and undisplayed constraints cannot be assessed to determine if they hold.
Automation behavior should be continuously visible and explainable to user. 
User should be able to extract meaning from the information display quickly, as in the case of Swarmish and the robot-as-pixel desgins.
As the system changes, the changes and predictions should be highlighted so that the user understands consequences of their actions. 

Once interaction some section of the process has been taken over by automation, the user operates primarily in the rules and knowledge areas, dealing with exceptions \cite{vicente2002ecological}.
The interface should allow direct manipulation of perceptual forms that map directly onto work-domain constraints and represent all of the information identified by the abstraction hierarchy. 
In a swarm context, this means displaying functional information in such a way that the user can move across the hierarchy from individual swarm bots to high-level swarm-wide tasks, and interact at all levels to control the swarm. 
EID is well-positioned to deal with emergent behavior, because the emergent behavior of the entire system is present at the functional level, but is composed of actions at the physical level.  
%"Reciprocity of user and environment, represenative design of experiments and evaluations, primacy of perception, and start with analyzing the environment". Information about a system is abstracted across a hierarchy, high level is functional information ("This plant makes reciprocating flange whompers"), low level is physical information ("This is a fleeble grommet, it is located in the engine room"). Hierarchy is over work domain, not task. EID should encourage skill and rule-based behavior (for normal operations) while also allowing knowledge-based behavior (to solve unanticipated problems). 

Previous work in multi-touch interfaces directly relates to EID by providing both an omniscient camera view for direct manipulation of the high-level, functional actions of the entire swarm, and the ability to move down the hierarchy to control individual swarm members \cite{Micire:2009:ANG:1731903.1731912}.


\subsubsection{Metrics for the Behavior of Swarms}

Emergent behaviors arise from the interactions of actors with each other and the world around them. 
In the face of uncertainty in the world, the behaviors will also become uncertain. \todo{BRING TRUST WORK IN}
Programs synthesized to guide the swarm should be designed to be robust against failure or degradation of swarm members. 
The heterogeneity of the swarm may also be leveraged to increase its robustness against failures of individual nodes or alterations of the environment. 
Because heterogeneity increases the dimensionality of the solution space for program synthesis, it may adversely affect the performance of the program synthesis and the swarm's runtime convergence to the desired state.

In order to determine the quality of the behavior of the swarm, its efficacy at performing tasks must be measured. 
Harriot et al propose that metrics for measuring the interaction of humans and swarms differs significantly from the interaction of humans and individual robots \cite{harriott2014biologically}.
Nine classes of metrics are proposed. 
The most obvious differences from individual robot control metrics are the ideas of leadership within the swarm, and macromovement of the swarm. 
Human attributes - how the human interacts, trust, intervention frequency, etc. \\
Task performance - ability to accomplish task, speed, accuracy, cost \\
Timing - Command diffusion lag, behavior convergence \\
Status - Condition of the swarm, battery life, number of functioning members, stragglers \\
Leadership - Interaction between special members of the swarm and others, how well leaders are followed \\
Decisions - How actions are taken, likelyhood that the correct action is chosen \\
Communication - Speed, range, network efficiency \\
Micromovements - Motion of individual swarm elements relative to each other \\
Macromovement - Motion of the overall swarm, flocking, elongation, forming fancy shapes \\ 
Harriet et al also put the estimated transition point between multi-agent control and swarm around 50 individuals. 
Above that threshold, human interaction may be able to remain focused on macro level behavior, influencing the overall behavior of the swarm rather than control of individuals. 

Swarms have more uncertainty, because the reliability of individual robots is low; and higher attentional demands because there are many robots. 
It may be that above some threshold, the attentional demand will drop again, as the group is no longer treated as a large number of individuals, but as a single group. 
The user interface may be able to drive this re-imagining, and convey other information, by depicting the group in different ways \cite{manning2015heuristic}.
The base case is to simply display all the units as individuals. 
Other approaches include an amorphous shape covering the area occupied by the swarm, an amorphous shape with density shading and motion arrows, the fields of influence for leaders in the swarm, and the web generated by the flow of information within the swarm. 
Considered as a whole, the swarm has properties, such as center of gravity or flock thickness, that do not exist in individual robots. 
Views of these properties may assist the user, for example in determining what areas have insufficient robot density for a thorough search operation. 
%A lot of Dr. Adams' work in HSI was under ONR Award N00014-12-1-0987

If the user is unconcerned with the functioning of individual swarm members, so long as the swarm as a whole remains functional, the UI may simply drop malfunctioning individuals from view. 
This handling of error conditions on individual swarm units fits with the assumption that the swarm as a whole achieves robustness through redundant expendable units, while also allowing the human user to have a rough idea of how the situation is developing by watching the cloud shrink. 
Do long as progress appears to be being made on the mission, the user might let underperforming units slide. 
The supervisory system might not even announce when units are lost, until it starts to affect performance.  
In the limit, the swarm could be treated as a gas, and for tasks such as diffusion over an area, the performance of the swarm would be compared to the behavior of an ideal gas \cite{jantz1997kinetics}.
The addition of sensors and computation would then allow the robots to outperform a gas at tasks, and so achieve higher scores on a task-oriented metric than a gas could attain. 
 
%$\Rightarrow$ Is trust even a factor with a swarm? Trust of swarm as a whole vs. individual.
%$\Rightarrow$ What level is a swarm expected to operate at? At the swarm-as-a-whole level, it's more or less level 1, human tells swarm what to do. At the individual robot level, it's 7 or higher. 
%$\Rightarrow$ Is it a valid assumption that the swarm is never teleoperated? What is the use case for being a single ant or bee, especially out of thousands of units?
%$\Rightarrow$ Look into MiDAS (Mission Displays for Autonomous Systems). 
\subsection{Conversion of User Tasks into Robot Programs} 

In order to be truly decentralized and emergent, the robots must each have a program that governs their behavior without reference to a central authority. 
It is assumed that some central command point exists, as the human user can only be present in one location, but the data stream sent from the command point consists only of a program for each robot. 
This assumption is intended to support robust operation in a dynamically reliable communication network, where robots can be programmed as they become reachable. 
The data stream returned to the command point from each robot is expected to be a telemetry stream, which may be erroneous or absent. 
Again, this assumption is in support of robust operation on a unreliable network. 

In the abstract case, it is clear that any problem that can be solved by agents with homogeneous software can also be solved by agents with heterogenous software. 
Assume that for N agents, there are N distinct programs, one per agent, that can, given each agents' location and state, be executed so that the overall final state of the system arrives at the desired result. 
This is the heterogeneous software case, but can be converted to the homogeneous software case by producing one program, consisting of each of the N distinct programs from the heterogeneous case, plus a conditional control structure that executes the required program based on the robot's location and state.  
From a biomimetic standpoint, this is similar to asking if worker ants learn. 
If they learn, then the ants are hardware-homogeneous and software-heterogeneous, but if the worker ants cannot learn, then they are both hardware and software homogeneous. Either way, they can clearly perform the tasks required of worker ants. 

A concept of the swarm as a whole as a programmable entity runs into trouble with reliability. 
In conventional compilation, assuming the compiler is correct and the computer is correct, the compiled binary does what the source code says. 
Robots interact with the real world, which is much less likely to be ``correct'' in the same sense a compiler can be asserted to be. 
Programs for swarms are only going to be functional within some probabilistic grounds and assumed conditions. 
This requirement indicates that the situation has to be at least somewhat known ahead of time, so that the robots will all receive programs that allow them to perform the task.
In the ideal case, the emergent action of all of the robots interacting with the environment causes them to perform the task. 

\subsection{Swarm Software Development Methods}

Because the conversion of the specification of desired behavior for the swarm into individual programs for the swarm controllers is still an open question, it is necessary to understand the current methods used in the development of programs for swarm robots. 
Much of swarm robotic development follows the usual model of software development. 
Starting from a desired functionality, the developer writes a program that they think will provide that functionality.
The program is then tested, in simulation or on real robots, and its behavior is observed. 
The programmer then modifies their program to account for any observed difference between the desired function and the system's behavior. 
This loop of coding, testing, and coding again is repeated until the system behaves as expected, or the programmer graduates. 

\subsubsection {Language Approaches}

One approach to the conversion of the command language to programs for the robots is to define a transformation from the command language to executable code that can be codified into a compiler. 
As a consequence, input in the command language defines a program which is compiled and loaded onto the robots. 

One possibility is to define a command grammar like the programming environment Tierra. \cite{ray1991approach}
In Tierra, there is no such thing as an invalid program. 
All sequences of the existent symbols are regarded as executable programs, although some are more functional than others. 
While this does prevent the possibility of the user becoming frustrated by being told that their commands are invalid, the ease of creating valid, but undesirable commands creates other problems. 
Good user interface designs permit easy undoing of an undesired command, but with the physical world, this can be difficult. 
It is far easier reset a simulation of a robot than it is to recall a real robot from falling down the stairs. 
Furthermore, a command language with no invalid commands runs into the same problem as free-space gesture interfaces: there is no way for a user to be idly present to the interface.
Every motion is interpreted as something that must be acted upon. 
%Is it possible to build a compiler that converts user commands in task space into programs for robots without having to code in assumptions about the qualities of the robots? Does the compiler have to account for global properties (e.g. GPS denial)? Can the compiler account for robot-local properties (e.g. battery life, motor failure)?

\subsubsection{Amorphous Computing}

Amorphous computing (AC), also called spatial computing, is computation using locally-linked and interacting, asynchronous, unreliable computing elements dispersed on a surface or throughout a volume \cite{abelson2000amorphous}. 
The motivation for AC is that while it may be possibly to produce arbitrary quantities of ``smart dust'', it is not possible to ensure that it all works well and is precisely located, especially in real-world applications.
The goal of AC is to get useful work out of such materials, despite uncertainty as to their reliability and location. 
Smart dusts are also the limit-case, in terms of scale, for swarm robotics, and if AC promises to get useful work out of smart dust, then it also has some applicability to larger swarm robots.

Growing Point Language (GPL) is a programming language for determining the large-scale behavior of an AC medium.
The abstractions of GPL include ``growing points'' which move through the medium, affecting the state of the computational points they pass, and pheremones in the medium which control the motion of growing points.
The abstractions used in GPL are very similar to those that guide the development of biological systems, such as the phototropism of plants and the chemical gradient signaling that coordinates morphogenesis in developing organisms.   
Importantly, GPL does not make any prior assumptions on the location of the particles in the system, or robots in the swarm, aside from that they are sufficiently dense in the medium. 
For swarm robotics, this is an important quality, as precise localization may not be available. 
 
"Proto" is a language for a continuous plane spatial computer. Proto paper also lists TinyDB as a "database view of the devices making up the computer", *LISP, and Regiment. Continuous time evolution approximated in gamma calculus and P-systems. 

"Infrastructure for Engineered Emergence on Sensor/Actuator Networks" \cite{beal2006infrastructure} Devices share internal state with local neighborhood, all run the same code bu can diverge due to neighborhood influences, sensor values. Proto is a stream-processing language, program is a directed acyclic graph with a single root (the output stream), composing programs is placing nodes into the graph. Proto is strongly typed, but with type inference rather than explicit/static typing. Doesn't cover conversion of a task into code. 

There are several languages intended to program amorphous computers. 
"Proto" is a language for a continuous plane spatial computer \cite{correll2009ad}.
Because the devices are distributed over a plane, the difficulty in communicating between any two devices is a function of the distance between them, much as with RF or other radiative communications.
In Proto, the behavior of regions of space is described by the programmer, and the description is transformed into local actions for the network of devices. 
Because devices have a size in the real world, and space between them, the devices cannot not have a one-to-one mapping with the space, but instead perform an approximation of the desired behavior. 
Swarm robots are mobile, so some swarm algorithims can be implemented as a description of constraints on the robot's state, such as "the robot must have communications links to no more than 2 and no less than 1 other robots", and a command to move randomly unless the constraint is satisfied. 
Within a bounded environment, such an algorithm can be shown to converge \cite{correll2009ad}. 

Proto also has considerable appeal as a programming language for swarm control development because of the layered mapping from behavior of regions at the global level to programs for discrete points at the level of individual devices \cite{beal2006infrastructure}. 
If user interface interactions can be interpreted as indications of desired behaviors displayed over spatial regions, then conversion of those behaviors into programs in Proto may be automatable. 
As a result, the user can create control programs for each robot without having to learn Proto themselves.  

Growing Point Language (GPL) allows the specification of topological patterns in an amorphous computer, and so can also be used to specify the distribution of swarm robots in a space \cite{nagpal2004engineering}. 
GPL is inspired by the morphogenic controls present in biological organisms, which use gradients of chemicals called morphogens to dictate the development of cells \cite{turing1952chemical}.
The name GPL arises from one of the language's main abstractions, the growing point. 
The growing point is the location of activity within the amorphous medium, at which local agents are changing their state. 
Initially, all agents have the same state and program, with a few exceptions that serve as seeds for the growth to begin. 
If the pattern is not required to be fixed at a particular location, even the seeds could be undetermined initially, and elect themselves via a method such as lateral inhibition. 
During the execution of the GPL program, each agent chooses its state based on the presence of pheromones, which are morphogens with limited range. 
Range limitation on morphogens propagating between robots is set using a TTL (Time To Live) counter that propagates with the morphogen, and is decremented with each hop in the communication network. 
When the TTL hits zero, the morphogen message is no longer propagated. 
By controlling the production or propagation of morphogens within the amorphous medium, complex patterns can be developed. 

Origami Shape Language (OSL) uses the abstraction of a foldable sheet to form shapes, inspired by both origami and the folding of epithelial cells during the development of biological organisms \cite{nagpal2004engineering, nagpal2001programmable}.
Regions and edges on the sheet can be defined by propagation of morphogens, and folds along the edges result in the development of the final form.
Because of the use of morphogens and local communication between the agents on the sheet, there is no need for a global controller to dictate the development of the final form. 
Further, because the high-level description of the desired form does not involve abstractions of the underlying modules, OSL could operate on interlocking modular robots, actuated flexible materials, swarms, or other kinds of computational media. 
In fact, the flexible sheet could be assumed to be virtual, and the resulting motions of the sheet could be translated into motor commands to configure swarm robots into specific arrangements in space. 

%  Proto paper also lists TinyDB as a "database view of the devices making up the computer", *LISP, and Regiment. Continuous time evolution approximated in gamma calculus and P-systems. 
%"Infrastructure for Engineered Emergence on Sensor/Actuator Networks" \cite{beal2006infrastructure} Devices share internal state with local neighborhood, all run the same code bu can diverge due to neighborhood influences, sensor values. Proto is a stream-processing language, program is a directed acyclic graph with a single root (the output stream), composing programs is placing nodes into the graph. Proto is strongly typed, but with type inference rather than explicit/static typing. Doesn't cover conversion of a task into code. 
%$\Rightarrow$ Look into *LISP, Regiment, gamma calculus, P-systems, Functional Reactive Programming, Gooze

"Combining Amorphous Computing and reactive agent-based systems: a paradigm for pervasive intelligence?" David Servat, Alexis Drogoul. Eventually everything will have an embedded, networked processor. Everything will continue to be gnarly: high heterogeneity in terms of software, processors, use cases, controllers and users. Things should still behave in a useful, coherent, predictable way. Requiring users to constantly manage them is not that. Amorphous computational model: computational units, which may be faulty, randomly located in a dynamic environment. Homogeneous programming, limited communication radius. Paper holds that most research in the area is computational models (e.g. ant colony optimization) or communications methods. Hop-count based gradient methods, bio-inspired based on quorum sensing. ``AC Hierarchy" for performing computing tasks ``simplifies programming with high-level abstractions". 

"Software Eng. For Self-Adaptive Systems" (Is a 270 page book, maybe skim later?)

``Improving Efficiency in Mobile Robot Task Planning Through World Abstraction'' \cite{galindo2004improving} Hierarchical world representation and operating at multiple levels of that hierarchy to reduce dimensionality of the problem space. Successive refinement from broad goals to behavioral primitives. Primitives are available for swarm robotics, but overall actions take place over the swarm. How to actually accomplish the translation is a bit tricky. Even GPL and OSL work from a desired known final state to a behavior specification, so task specification would have to be based on a description of the final state of the system. 

"Notes on Amorphous Computing" Jacob Katzenelson. \cite{katzenelson2000notes} Doing differentiation and integration on amorphous computers. 

$\Rightarrow$ Is it useful to share sensor information in a broadband rate-coded way, e.g. spike trains? Neuronal computation seems a bit like the limit case of amorphous computing, except that the network isn't radius-bound. 

Many applications that are proposed for swarms are similar to foraging search, in that the robots spread into an area in order to locate and converge on an objective. 

User interface allows the user to define an overall desired distribution of the robots, position and directions, and then the control software compiles that into a representation in e.g. GPL or OSL that gets compiled into programs to run on the robots. This isn't a continuous controller, more fire \& forget. Could have robots with a main behavior of successfully navigating the world and other behaviors (search and rescue, track target, etc.) as riders. Sort of like putting sensors on existing bees, and then collecting the data while they do bee stuff. Paper is mostly on lightweight formal methods and policy checking. Could be used at a policy level for a swarm, e.g. to limit overall power use, but adds another layer of behavior that can alter emergency. 

Mobile software specs: "Mobile UNITY [Roman97], the Distributed Join-Calculus [Fournet96], and Mobile Ambients [Cardelli98]."

"A much more ambitious goal is to produce the device programs for a primitive swarm program from the high-level behavior description. It is not realistic to hope that we could completely automate the process." This is pretty much what we're talking about doing. 

Lists tuple spaces, amorphous computing, as potential enabling technologies for swarm programming. 

"Continuous Space-Time Semantics Allow Adaptive Program Execution" \cite{bachrach2007continuous} Jonathan Bachrach, Jacob Beal, Takeshi Fujiwara. Spatial computer model, devices that interact depending on their location in space. Computer is "programm[ed]...as a continuious space". Swarm robotics as a special case of the spatial computer. "Proto" is language for a continuous plane spatial computer. Operations act as reduce() or similar over local area or history, feedback loops provide state. Paper provides emergent computation of firefly-style synchronization (coupled oscillators, so perhaps some elements of swarm computing can be expressed as variably-coupled oscillators in combination with driving signals derived from internal states and sensor inputs. This would in some ways be the ultimate evolution of the BEAM "Robot Jurassic Park" idea from the '90s, where each robot is sharing some of its internal state via coupled oscillation with the other robots in the park, e.g. to communicate the presence of light sources for ``food", the presence of self for collision avoidance, etc.)

"The mechanism for binding sensors to names is implementation dependent, as are the value when sense is applied to an unbound name and the result of multiple streams being sent to the same actuator." So, uh, the actual interaction of the program with the world is ``implementation dependent"? Good, knowing what your code does is for suckers. 

"A Formal Approach to Autonomic Systems Programming: The SCEL Language" \cite{nicola2014formal} ROCCO DE NICOLA et al. Self-managing autonomous ensembles of systems. Only related to swarm robotics in the sense that it is for more than one computer. Assumes complete localization and inter-swarm communication. 

"Scripting the Swarm: Event-based control of Microcontroller-based robots" \cite{magnenat2008scripting} Stephane Magenenat et al, EFPL. Introduces ASEBA, an event-based control architecture. Reflashing e-puck takes a minute, doesn't scale to a bunch of robots unless you can do it to all robots at once. Concurrent debugging is impossible. Programs compiled to bytecode, react to incoming sensor data (e.g. message from other robot, impending collision, etc.). Could compose swarm behavior by deciding reaction priorities, tuning priorities in reaction to environment. Still doesn't describe task-to-program compilation. Does permit differing code between robots, which won't scale to 1k+ robots.

\subsubsection{Compositional Approaches}

One possible approach is the composition of programs from behavioral primitives, such that some combination of the primitives results in the emergence of the desired behavior. 
In ``Occlusion-Based Cooperative Transport with a Swarm of Miniature Mobile Robots", individual robots cooperate to push an object to a beacon based on simple behaviors \cite{chen2015occlusion}. 
If the view of the beacon is blocked, the robots push the object. 
If the robots can see the beacon, they wander and avoid obstacles. 
The sum of the two behaviors results in a net pushing force on the side of the object opposite the beacon, which moves the object to the beacon. 

One abstraction of a swarm for programming is a collection of mobile, physically situated processors communicating over an ad-hoc network \cite{evans2000programming}. The behavior of the swarm is emergent from the behavior of individual units, and is  resilient against "misbehaving members".
Two approaches to programming this abstraction of a swarm are to compose program from individual behavior primitives, or to synthesize unit programs based on description of environment and desired behavior. 
%Analysis is approximate, rather than general proof of properties.

In the compositional approach, any combination of behaviors is a valid program, but might not be a good/useful program. 
Evans lists disperse (no other nodes within distance d), general disperse (no more than n nodes within distance d), clump/cluster, attract to location, swarm in a direction, and scan area as primitives for motion, and broadcast, partialcast, and unicast as methods of swarm communication.
Behaviors in a program can be composed to operate serially or in parallel, and divided across the swarm or synchronized across the swarm. 
Most biological swarms, such as insects, could be argued to have a priority based parallel evaluation, where different behaviors become active as context requires.
Higher organisms, including humans in large groups, engage in serial and synchronized behaviors. %TODO is there evidence for one kind or the other? 

Another proposed catalog of behaviors for swarm control bases the simple behaviors on pheremones or chemical sensing in single cells \cite{nagpal2004catalog}. 
The proposed behaviors are the use of gradient sensing for position and direction information, local inhibition and competition, lateral inhibition for spatial information, local monitoring, quorum sensing for timing and counting, checkpoint and consensus sensing, and random exploration. 
The first five are common in amorphous computing, but the last three are not. %TODO what are their uses?. 
These senses are sufficient for relatively complex behaviors. 
Quorum sensing is used to detect whether the local agent count is sufficient for a task. 
This allows just-in-time allocation of robots to tasks rather than pre-allocation when the task is designed. 
This may be more robust against failures of individual robots, as it uses the robots that are in the right place at the right time, rather than waiting for specially assigned robots. 
However, under sufficiently bad conditions, a sufficient quorum may never arrive, deadlocking the task. 
In combination with domino timing, where completion of each phase triggers the next, this could then deadlock the entire process unless another mechanism detects and corrects it.

It has been demonstrated that a swarm can perform construction tasks using only local sensing and no communication \cite{wawerla2002collective, bowyer2000automated}.
However, the addition of communication between systems and memory of the state of the world will improve the efficency of the system.
The system under discussion was developed to have the task implied by the behaviors available for the agents, rather than generating the program from a higher-level specification, such as the form of the structure to be built.

Another compositional method for programming robots proposes that the behaviors can be seperated into classes, such as motion, orientation, and so forth \cite{mclurkin2004stupid}. 
Among these behaviors are ``primitives'' such as several forms of clustering, which other, later works have treated as an emergent behavior itself, arising from more primitive primitives. 
The variable granularity of the primitives available to compose swarm control programs seems to point to a hierarchy of control elements, with perhaps single motor operations at the bottom, and an increasing composition of elements to create more and more complex behaviors.
Demos written to run on the swarm called multiple primitive behaviors, with parameters such as degrees of bearing and centimeters of proximity. 
The behaviors ideally run concurrently, and some of them respond to sensor inputs. 
The output of behaviors is whether they are running, translational and rotational velocity, and LED configuration. 
Subsumption and summation are used to arbitrate between behaviors of differing priorities. 

McLurkin's swarm had many properties that would be highly desirable in a swarm intended to operate robustly in a potentially hostile environment. 
For example, the robots did not have unique identities, and so were interchangeable as far as any algorithms involving them were concerned. 
It might be useful for robots to have global, local, or no names, or temporary names, but any algorithm that depends on them may end up depending too much on a particular robot. 
However, the swarm did not have any learning within the robots, and relied on the configuration of the environment to inform the behavior of the robots. 
As a result, ``bad environments" could elicit pathological behavior from the swarm. 
In real use cases, there is no such thing as a ``bad environment". 
The environment simply is, and so if a robot fails in an environment, it is the robot and not the environment that is bad. 

The absence of global IDs for the robots does pose a problem for determining if all of the robots are reachable from all of the other robots, in other words, if the network is a single connected component. 
With global IDs, the absence of some subset of them indicates that the network is divided. 
Without global IDs, the network may be divided but the robots will be unable to detect it. 

Unfortunately, much of the design of programs for swarm robots consists of iterating between coding and observing behavior of the system in an ad hoc process \cite{palmer2005behavioral}. 
Emergence is the transition between two levels of behavior. 
At the base level is the behavior explicit in the program and the actions generated by it in the environment. 
At the higher, emergent level is the implicit behavior of the program, as the sum of the results of the robots' actions. 
Babarilla et al propose a taxonomy of behaviors that separates spatially organizing, navigation, decision-making, and ``other'' behaviors into their own groups \cite{brambilla2013swarm}. 
These behaviors are higher level, swarm-wide behaviors, rather than individual behaviors of individual robots. 
If it is assumed that these are the emergent behaviors of the system, then perception of the emerging behavior can in turn affect the base level behavior of the robots, providing feedback. 
This feedback can be used to increase the tendency of the behavior of the interacting swarm members to converge on a solution. 
By observing the state of the environment, swarm robots determine if they are satisfied with the local state. 
The dissatisfied members change their actions, but the satisfied units do not. 
However, this feedback alone will not stabilize, as environmental changes are not required to increase the total of satisfied robots. 
The longer a unit remains satisfied, the less easy it is for it to become dissatisfied. 
This bias helps prevent the disruption of partial solutions as the situation develops. 
However, it does require a local criteria for detection of a partial solution, and so cannot be used to solve problems whose progress is not locally visible. 

These emergent approaches do not have the robots perform all of their available actions all of the time. 
Instead, it is assumed that the behavior of each robot is controlled by its reaction to the environment around it, and possibly to signals from other robots, so that actions are only performed when they are required. 
As a result, user programs compiled from a higher-level representation could be a table consisting of possible values for the sensors, and the actions to undertake when those values are met.
``A Compositional Framework for Programming Stochastically Interacting Robots" \cite{napp2011compositional} provides a formal framework for the analysis of this type of compositional program, called Guarded Command Programming with Rates (GCPR). 
Robots are assumed to only have local sensing.
The guards of GCPR are conditions on the environment.
When a condition is met, the robot performs actions at a given rate. 
%This is a lot like behavior-based robotics.
In the concurrent case, this is modeled as each action happening one at a time, but in random order. 
On a real swarm, the actions would take place in parallel, but the concurrent model is more amenable to analysis. 
To determine if a set of actions will be successful, it is required to ensure that for all orderings of all actions, the final state space of the swarm is the desired final state. 
Failed behaviors are modeled as a program that doesn't do what the other program it is composed with does. 
Correct programs are those that reach the target state with probability one, even when composed with bounded failures. 
Once the target state is reached, the program is assumed to alt, so while the final state may be reached very slowly, but once it is reached, it is not left. 
In the GCPR models, the time to execution of an action is stochastic, but in the real-world case of noisy or imperfect sensors, the variable time to execution of a guarded behavior would be caused by the imperfection of the robot's ability to detect that the guard was satisfied. 
In the real world, we get stochasticity for free. 

%$\Rightarrow$ Get Klavins et al 2k4, apparently on algorithmic approach mapping local to complex global behavior.

\subsubsection{Evolutionary Composition}

Genetic algorithims (GA) have been proposed as a possible way of directing the composition of behavioral primitives into programs \cite{palmer2005emergence}.
However, it is difficult to automatically extract from the know be behavior of the system an overall understanding of the progress it is making on the task. 
Ideally, it would be possible to recognize and evaluate performance on sub-problems. 
Worse, without a time bound on solving a problem or a way to calculate progress, it is impossible to tell if a program has failed, or has merely not succeeded yet.
For example, assume a program's intended purpose is to gather units of a resource at a goal. 
If the program merely moves the units stochastically, sometimes they will enter the goal, creating an appearance of progress. 
However, it may be vanishingly unlikely that all the units will randomly happen to be in the goal at once. 
Even a program that cannot find the goal, and so will never put any units in it, behaves exactly the same as a perfect resource-gathering program that just hasn't moved any units to the goal yet. 
Meta-rules that govern selection of rules at the individual agent level depending on agent context. 
Nested hierarchies of systems and their contexts, but can be collapsed to an arbitrary level and its next level up (higher has no downward effect, lower can be considered part of base level). 
All information for the actor is local (its own sensors and memory, can possibly share with other actors). 
Uses aspect-oriented-programming (AoP) to recognize method invocations for single-agent actions as an aspect (e.g. maintain-distance and move-random forming a dispersion aspect).
Analysis of the behavior of the swarm looking for good patterns and trying to develop code that does not yet exist to create behaviors that have not yet been observed is REALLY HARD, so have humans do it. 
This is a design process, not an automation process, but indicates a direction for possible ML approaches (which seems like a return to the GA approaches the authors dismissed earlier, possibly as less-directed than human intervention.). 
This requires multiple design/build/test loops for each activity the swarm tries to do.
Perhaps have it learn forageForObject until object is found, surroundObject until object is surrounded, then "pushObject" until object is where it should be, then disperseRobots until the object is left alone again. 
Humans can do ``what-if" experiments on individual robots (so can the computer, and faster, assuming some evaluation function is available). 
Swarm algorithims extracted from Virtual Human Swarm (VHS) having people wander around a gymnasium. 

Determining how the behaviors should be composed for an individual swarm robot's controller is difficult. 
One approach is to set some intuitively reasonable values and then test the behavior of the software on the swarm. 
This may not scale well to large swarms, depending on the technology used to distribute the software. 
If each robot needs to be individually reprogrammed, then the time required for distributing a software update will grow with the size of the swarm, and eventually become unmanageable. 
Assuming the program can be swiftly distributed, then programming it is simply a matter of iterative software development, with each bug or undesired behavior accounted for in the next iteration. 
However, this approach assumes that the behavior of the swarm is being dictated by programmers, and expressed as a computer program. 
While it must be a program at some level, the behavior of the swarm must be easier and faster for non-programming users to express and control. 

It is possible to develop self-organizing behaviors for robots using genetic algorithms \cite{dorigo2004evolving}. 
This does offer a reduction in the amount of hands-on time spent programming, but it frequently comes at the expense of time spent waiting for the system to converge, or determining why it converged on a problematic solution. 
Performing the evolution simulation allowed the robots to develop behaviors such as aggregation, but early versions also allowed the evolved motion strategy to acquire a high fitness by spinning in place. 
For mutual motion when linked, robots could sense traction, allows negotiation of a common direction of movement by proprioception. 
Similar traction-based interactions allowed the robots to avoid obstacles and pull objects. 
Solutions discovered by genetic algorithms are also prone to overfitting. 
The swarms described in Dorigo et al decreased in performance when the number of robots involved in the swarm was changed from the values used to evolve the solutions, and when a more accurate physical model was used in the simulations.

Aggregation behavior has also been evolved in swarm robots, using a perceptron as the controller for the robots and evolving the perceptron weight vector \cite{bahgecci2005evolving}. 
Aggregation was chosen because it is a preliminary behavior primitive, which the swarm might engage in prior to doing some other task, such as moving an object, attacking \emph{en mass}, etc.
The resulting vector only controls aggregation behavior, so each behavioral primitive would require its own evolutionary development. 
Evolution is only useful if it is a faster approach than designing the behavior by hand. 

Quinn et al have developed controllers that allow robots to move into formation from random starting positions \cite{quinn2003evolving}. 
These controllers use local interactions and minimal sensing to achieve their goals. 
One point the authors make, which is not frequently mentioned in other work, is that while flocking or shoaling behavior is a relatively simple behavior to have emerge from robots who can detect the distance, postion, and velocity of the other nearby robots, it is actually quite hard to have robots do that detection. 
Because a specific behavior, motion in formation, was desired, the fitness function used to evolve it was specified in terms of metrics related to the behavior. 
Task-specific fitness functions are also found in later work on evolution of swarm robot behavior, which seems to indicate that evolution of behaviors in swarm robots may only be a time-complexity tradeoff. 
The complexity of directly specifying the task is reduced to the complexity of describing the results in the fitness function, but the time required increases to that required by the evolutionary iterations. 
Additionally, the ad hoc iterative process of creating emergent behaviors is replaced by an ad hoc iterative process of creating fitness functions.
Developing novel behavior in the field by converting user specifications of the behavior of the swarm into a fitness function for a genetic algorithm are unlikely to yield results in a timely manner. 

Interestingly, some of the work in evolvable controllers leads to inter-robot communication as one of the emergent properties of the evolved controller \cite{quinn2001evolving}.
In order to move as a formation, one of the robots must be the leader, but there is nothing in the fitness function or any of the other code that designates roles for the robots. 
Instead, the selection of the leader arises from the evolutionary development of the controllers, and is present in the controller as a response to a particular series of stimuli. 
Genomes that did not encode such a symmetry-breaking reaction never developed a leader-follower distinction, and so failed to move in formation, and so received low fitness scores. 
For the follow-the-leader task, genetic variation among the robots increased fitness more readily than having all robots share the same genome \cite{quinn2001comparison}.
The condition where all robots shared the same genes was called ``clonal", while each robot having its own genome was ``aclonal".
Oddly, while one would expect that the aclonal condition would result in a specialization, with each robot developing a genome that performed either the leader or follower role well, the aclonal condition developed robots which could perform both roles. 
Quinn hypothesizes that while the clonal condition had to evolve roles and an allocation mechanism simultaneously, the aclonal condition could specialize the roles during early evolution, and then develop an arbitration mechanism to select roles.

\subsubsection{Pheromone Approaches}

Another approach used to control the behavior of swarm robots is based on the chemical signaling used by animals for communication. 
The chemicals are called pheromones, and in pheromone robotics, they are usually simulated or ``virtual'' pheromones, rather than attempting to use real chemicals and chemical sensors. 
Pheromones are assumed to have a set of characteristics which robots can sense. 
For example, a robot may emit a pheromone which diffuses into the environment, so distance from the robot can be determined by the strength of the pheromone, and approaching or avoiding the robot may be accomplished by moving up or down the gradient of pheromone strength. 
Different pheromones may have different characteristics. 
The presence pheromone may be constantly emitted by the robot, but a robot which is searching for something may emit a ``search marker'' pheromone that lingers in the area after the robot leaves. 
Other robots, on entering the area, would detect the pheromone and know that searching this area again would be fruitless. 
If the object of the search can move, the pheromone could diminish as a function of time, so areas that have not been searched for a long time become unmarked and may be searched again. 

Pheromone approaches can guide the construction of objects, even if the individual swarm members have no memory and only local perception\cite{mason2003programming}. 
The agents engaged in the construction move at random, and take actions governed by their individual perception of environment at present time. 
The agents can release and react to pheromones in the environment, and so there is an implicit communication via stigmurgy, but no explicit agent-to-agent communication. 
A set of environmental triggers is "coherent if no stage in the building process can be confused with an earlier stage by making only local observations, thus obviating the need for centralized control".   
By specifying such a coherent set of triggers, the specification for the building can be expressed to the swarm. 
However, Mason does not propose a user interface for creating this specification, saying instead that "Other future work includes programming construction swarms by specifying the target structure directly, letting a compiler infer the corresponding rule-set (if one exists)."

The addition of directional communication for the messages that convey virtual pheromone information allows easy determination of the direction of pheremone gradients \cite{payton2001pheromone}.
Rather than directly diffusing in the space as a chemical would, hop counts in the network of robots simulate diffusion. 
Because routes may be of different length, the message with the lowest hop count is assumed to be the truest indication of minimal distance within the network. 
Payton provides algorithms for budding with growth inhibition to allow a swarm to explore while remaining connected as a network. 
Rather than modeling the world based on the incoming messages, the content of the pheromone messages and the network behavior as a whole serves as a model of the world, mapped 1:1 onto the real environment. 
While it is possible to build a set of behavioral primitives out of pheromone signaling and associated behaviors, controlling the swarm to perform a task with these primitives is still done by hand \cite{payton2003compound}.

\section{Conclusion}

The various approaches to development of swarm robot control programs show that a wide variety of approaches can still result in robust controllers for swarm robots. 
Previous work in HRI shows that multi-touch interfaces will allow a scalable and direct mapping between the desires of the user and sequences of commands to the swarm. 
While swarm hardware is not yet at a point where very complex computation may be pushed directly to the swarm nodes themselves, that time is not far off. 
Until computational power in the individual swarm units does reach the levels required for complex computation, virtualization of computing resources can provide an adequate test environment for the development of swarm control algorithms at modest requrements in terms of space and power consumption. 
\unsure{develop conclusion more, perhaps restate some of research questions?}

\bibliography{swarm.bib}
\bibliographystyle{plain}

\end{document}

%Papers to read

%Cooperative interaction of walking human and distributed robot maintaining stability of swarm 

%Development of IR-based short-range communication techniques for swarm robot applications 

%The Wanda Robot and Its Development System for Swarm Algorithms

%Stability of swarm robot based on local forces of local swarms

%Swarm robot pattern formation using a morphogenetic multi-cellular based self-organizing algorithm 

%A particle-swarm-optimized fuzzy-neural network for voice-controlled robot systems 

%The I-SWARM project 


